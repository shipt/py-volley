{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Volley is a lightweight and highly configurable message stream processor for Python. Repository : https://github.com/shipt/volley Use Volley to quickly build lightweight message queue processing microservices. Write your applications and add a few lines of code to integrate with technologies like Kafka. Volley has built in connectors for Confluent Kafka and Redis Simple Message Queue . It also provides serialization implementations in MessagePack and orjson , as well as data validation with Pydantic and Prometheus metrics. Example \u00b6 To demonstrate, let's create an application with two worker nodes. One consumes from Kafka, finds the maximum value in a list then publishes it to Redis. The other consumes the message from Redis - if the max value is > 10, it logs to console otherwise it constructs a new list and publishes to the same Kafka topic. For most simple cases, you can use a profile to define how your application should consume from or produce to a topic, as well as define handlers for serialization and data validation. Setup Kafka and Redis \u00b6 We'll use these for our queues. docker run -d -p 6379 :6379 redis:5.0.0 docker run -d -p 9092 :9092 bashj79/kafka-kraft Publish some sample data \u00b6 Let's put a single message on the Kafka topic. Our sample application will process it. import json import os from confluent_kafka import Message , Producer def acked ( err : str , msg : Message ) -> None : if err is not None : print ( f \"Failed delivery to { msg . topic () } , message: { msg . value () } , error: { err } \" , ) else : print ( f \"Successful delivery to { msg . topic () } , partion: { msg . partition () } , offset: { msg . offset () } \" ) producer = Producer ({ \"bootstrap.servers\" : os . getenv ( \"KAFKA_BROKERS\" , \"localhost:9092\" )}) producer . produce ( topic = \"my.kafka.topic.name\" , value = json . dumps ({ \"my_values\" : [ 1 , 2 , 3 ]}), callback = acked ) producer . flush ( 10 ) print ( \"Published single message\" ) Configure queues \u00b6 Let's put the configurations in ./my_config.py . These configurations our worker nodes how to consume and produce to each queue. The profile provides presets for serialization and connectors and the Pydantic model handler. The InputMessage and OutputMessage will be used for validating the data we consume. The confluent profile uses orjson as the default serializer. # my_config.py import os from typing import List from pydantic import BaseModel # define the schemas for the first and second worker nodes. class InputMessage ( BaseModel ): my_values : List [ float ] class OutputMessage ( BaseModel ): the_max : float # define the configurations for the two queues, one in Kafka and the other in Redis. queue_config = { \"my-kafka-input\" : { \"value\" : \"my.kafka.topic.name\" , \"profile\" : \"confluent\" , \"data_model\" : InputMessage , \"config\" : { \"bootstrap.servers\" : os . getenv ( \"KAFKA_BROKERS\" , \"localhost:9092\" ), \"group.id\" : \"my.consumer.group\" }, }, \"my-redis-output\" : { \"value\" : \"my.redis.output.queue.name\" , \"profile\" : \"rsmq\" , \"data_model\" : OutputMessage , \"config\" : { \"host\" : os . getenv ( \"REDIS_HOST\" , \"localhost\" ), \"port\" : 6379 , }, }, } First worker node \u00b6 We'll put the first worker node in app_0.py This node will consume from Kafka and produce to Redis. Configure the Engine() to point to the input_queue and output_queues specified in our queue_config we defined earlier. The worker node is a function and it will receive a single object, which will be the message serialized and constructed into the data_model we specified in our queue_config . The worker node can publish to zero or many outputs. When electing to not rely on Volley to route your message to another queue, simply return a bool . True will commit offsets to Kafka (or delete the message from Redis), and False will leave the message alone. Volley will produce all the messages you return to it by returning List[Tuple[<name_of_queue>, message_object]] # app_0.py import logging from typing import List , Tuple from my_config import InputMessage , OutputMessage , queue_config from volley import Engine logging . basicConfig ( level = logging . INFO ) # the first node - reads from kafka and writes to redis app_0 = Engine ( app_name = \"app_0\" , input_queue = \"my-kafka-input\" , # one input output_queues = [ \"my-redis-output\" ], # zero to many outputs queue_config = queue_config , ) @app_0 . stream_app def kafka_to_redis ( msg : InputMessage ) -> List [ Tuple [ str , OutputMessage ]]: logging . info ( f \"Received { msg . json () } \" ) max_val = max ( msg . my_values ) out = OutputMessage ( the_max = max_val ) logging . info ( out ) return [( \"my-redis-output\" , out )] # a list of output targets and messages if __name__ == \"__main__\" : kafka_to_redis () Then run the first worker python app_0.py Note: Volley supports asyncio , just prefix your function with async : @app . stream_app async def my_fun ( msg : Any ): Second worker node \u00b6 The second worker node is much like the first, except it will read from Redis and conditionally produce back to Kafka, or just create a log. We'll add some logic to make that slightly more interesting. # app_1.py import logging from typing import List , Tuple , Union from my_config import InputMessage , OutputMessage , queue_config from volley import Engine logging . basicConfig ( level = logging . INFO ) # the second node app_1 = Engine ( app_name = \"app_1\" , input_queue = \"my-redis-output\" , output_queues = [ \"my-kafka-input\" ], queue_config = queue_config , metrics_port = None , ) @app_1 . stream_app def redis_to_kafka ( msg : OutputMessage ) -> Union [ bool , List [ Tuple [ str , InputMessage ]]]: logging . info ( f \"The maximum: { msg . the_max } \" ) if msg . the_max > 10 : logging . info ( \"That's it, we are done!\" ) return True else : out = InputMessage ( my_values = [ msg . the_max , msg . the_max + 1 , msg . the_max + 2 ]) return [( \"my-kafka-input\" , out )] # a list of one or many output targets and messages if __name__ == \"__main__\" : redis_to_kafka () Then run the second worker node. python app_1.py You should see the following in your two terminals ./app_0.py Received {\"my_values\": [1.0, 2.0, 3.0]} the_max=3.0 Received {\"my_values\": [3.0, 4.0, 5.0]} the_max=5.0 Received {\"my_values\": [5.0, 6.0, 7.0]} the_max=7.0 Received {\"my_values\": [7.0, 8.0, 9.0]} the_max=9.0 Received {\"my_values\": [9.0, 10.0, 11.0]} the_max=11.0 ./app_1.py The maximum: 3.0 The maximum: 5.0 The maximum: 7.0 The maximum: 9.0 The maximum: 11.0 That's it, we are done! See our example for a more in-depth example. Features \u00b6 Volley is production-ready, and provides the following to all functions that it wraps: Prometheus Metrics for system observability Data validation via Pydantic Built-in support for both Kafka and pyRSMQ (Redis) Serialization in JSON and MessagePack Extensible data validation and schemas, data store connectors, and serialization with plugins Optional dead-letter-queues for serialization and schema validation failures How-it-works \u00b6 Volley handles operations that need to happen before and after the processing in your function: poll the specified input_queue for new messages via a connector the connector passes message through serialization serialization passes the message through a user provided data_model for schema validation message is passed to your function, and your function messages returns zero to many messages back to Volley message passes through schema validation specified for the output_queue message is serialized for output_queue connector publishes message to output_queue Messages that fail either serialization or validation are routed to a dead-letter-queue , if specified. Users can also route messages to a dead-letter-queue the same as any other output queue.","title":"Volley"},{"location":"#example","text":"To demonstrate, let's create an application with two worker nodes. One consumes from Kafka, finds the maximum value in a list then publishes it to Redis. The other consumes the message from Redis - if the max value is > 10, it logs to console otherwise it constructs a new list and publishes to the same Kafka topic. For most simple cases, you can use a profile to define how your application should consume from or produce to a topic, as well as define handlers for serialization and data validation.","title":"Example"},{"location":"#setup-kafka-and-redis","text":"We'll use these for our queues. docker run -d -p 6379 :6379 redis:5.0.0 docker run -d -p 9092 :9092 bashj79/kafka-kraft","title":"Setup Kafka and Redis"},{"location":"#publish-some-sample-data","text":"Let's put a single message on the Kafka topic. Our sample application will process it. import json import os from confluent_kafka import Message , Producer def acked ( err : str , msg : Message ) -> None : if err is not None : print ( f \"Failed delivery to { msg . topic () } , message: { msg . value () } , error: { err } \" , ) else : print ( f \"Successful delivery to { msg . topic () } , partion: { msg . partition () } , offset: { msg . offset () } \" ) producer = Producer ({ \"bootstrap.servers\" : os . getenv ( \"KAFKA_BROKERS\" , \"localhost:9092\" )}) producer . produce ( topic = \"my.kafka.topic.name\" , value = json . dumps ({ \"my_values\" : [ 1 , 2 , 3 ]}), callback = acked ) producer . flush ( 10 ) print ( \"Published single message\" )","title":"Publish some sample data"},{"location":"#configure-queues","text":"Let's put the configurations in ./my_config.py . These configurations our worker nodes how to consume and produce to each queue. The profile provides presets for serialization and connectors and the Pydantic model handler. The InputMessage and OutputMessage will be used for validating the data we consume. The confluent profile uses orjson as the default serializer. # my_config.py import os from typing import List from pydantic import BaseModel # define the schemas for the first and second worker nodes. class InputMessage ( BaseModel ): my_values : List [ float ] class OutputMessage ( BaseModel ): the_max : float # define the configurations for the two queues, one in Kafka and the other in Redis. queue_config = { \"my-kafka-input\" : { \"value\" : \"my.kafka.topic.name\" , \"profile\" : \"confluent\" , \"data_model\" : InputMessage , \"config\" : { \"bootstrap.servers\" : os . getenv ( \"KAFKA_BROKERS\" , \"localhost:9092\" ), \"group.id\" : \"my.consumer.group\" }, }, \"my-redis-output\" : { \"value\" : \"my.redis.output.queue.name\" , \"profile\" : \"rsmq\" , \"data_model\" : OutputMessage , \"config\" : { \"host\" : os . getenv ( \"REDIS_HOST\" , \"localhost\" ), \"port\" : 6379 , }, }, }","title":"Configure queues"},{"location":"#first-worker-node","text":"We'll put the first worker node in app_0.py This node will consume from Kafka and produce to Redis. Configure the Engine() to point to the input_queue and output_queues specified in our queue_config we defined earlier. The worker node is a function and it will receive a single object, which will be the message serialized and constructed into the data_model we specified in our queue_config . The worker node can publish to zero or many outputs. When electing to not rely on Volley to route your message to another queue, simply return a bool . True will commit offsets to Kafka (or delete the message from Redis), and False will leave the message alone. Volley will produce all the messages you return to it by returning List[Tuple[<name_of_queue>, message_object]] # app_0.py import logging from typing import List , Tuple from my_config import InputMessage , OutputMessage , queue_config from volley import Engine logging . basicConfig ( level = logging . INFO ) # the first node - reads from kafka and writes to redis app_0 = Engine ( app_name = \"app_0\" , input_queue = \"my-kafka-input\" , # one input output_queues = [ \"my-redis-output\" ], # zero to many outputs queue_config = queue_config , ) @app_0 . stream_app def kafka_to_redis ( msg : InputMessage ) -> List [ Tuple [ str , OutputMessage ]]: logging . info ( f \"Received { msg . json () } \" ) max_val = max ( msg . my_values ) out = OutputMessage ( the_max = max_val ) logging . info ( out ) return [( \"my-redis-output\" , out )] # a list of output targets and messages if __name__ == \"__main__\" : kafka_to_redis () Then run the first worker python app_0.py Note: Volley supports asyncio , just prefix your function with async : @app . stream_app async def my_fun ( msg : Any ):","title":"First worker node"},{"location":"#second-worker-node","text":"The second worker node is much like the first, except it will read from Redis and conditionally produce back to Kafka, or just create a log. We'll add some logic to make that slightly more interesting. # app_1.py import logging from typing import List , Tuple , Union from my_config import InputMessage , OutputMessage , queue_config from volley import Engine logging . basicConfig ( level = logging . INFO ) # the second node app_1 = Engine ( app_name = \"app_1\" , input_queue = \"my-redis-output\" , output_queues = [ \"my-kafka-input\" ], queue_config = queue_config , metrics_port = None , ) @app_1 . stream_app def redis_to_kafka ( msg : OutputMessage ) -> Union [ bool , List [ Tuple [ str , InputMessage ]]]: logging . info ( f \"The maximum: { msg . the_max } \" ) if msg . the_max > 10 : logging . info ( \"That's it, we are done!\" ) return True else : out = InputMessage ( my_values = [ msg . the_max , msg . the_max + 1 , msg . the_max + 2 ]) return [( \"my-kafka-input\" , out )] # a list of one or many output targets and messages if __name__ == \"__main__\" : redis_to_kafka () Then run the second worker node. python app_1.py You should see the following in your two terminals ./app_0.py Received {\"my_values\": [1.0, 2.0, 3.0]} the_max=3.0 Received {\"my_values\": [3.0, 4.0, 5.0]} the_max=5.0 Received {\"my_values\": [5.0, 6.0, 7.0]} the_max=7.0 Received {\"my_values\": [7.0, 8.0, 9.0]} the_max=9.0 Received {\"my_values\": [9.0, 10.0, 11.0]} the_max=11.0 ./app_1.py The maximum: 3.0 The maximum: 5.0 The maximum: 7.0 The maximum: 9.0 The maximum: 11.0 That's it, we are done! See our example for a more in-depth example.","title":"Second worker node"},{"location":"#features","text":"Volley is production-ready, and provides the following to all functions that it wraps: Prometheus Metrics for system observability Data validation via Pydantic Built-in support for both Kafka and pyRSMQ (Redis) Serialization in JSON and MessagePack Extensible data validation and schemas, data store connectors, and serialization with plugins Optional dead-letter-queues for serialization and schema validation failures","title":"Features"},{"location":"#how-it-works","text":"Volley handles operations that need to happen before and after the processing in your function: poll the specified input_queue for new messages via a connector the connector passes message through serialization serialization passes the message through a user provided data_model for schema validation message is passed to your function, and your function messages returns zero to many messages back to Volley message passes through schema validation specified for the output_queue message is serialized for output_queue connector publishes message to output_queue Messages that fail either serialization or validation are routed to a dead-letter-queue , if specified. Users can also route messages to a dead-letter-queue the same as any other output queue.","title":"How-it-works"},{"location":"advanced_example/","text":"Advanced Example \u00b6 Run the pre-built example from the root level of this repository with make run.example You will need docker , and docker-compose installed on your machine. Kafka, Redis, and Postgres are each spawned in their own Docker containers defined in docker-compose.yml . The example consists of the Volley workers, which are functions described below. ./example/external_data_producer.py publishes sample data to input-topic Kafka topic. ./example/input_worker.py consumes from input-topic and publishes to redis_queue , RSMQ queue. ./example/middle_worker.py consumes from redis_queue RSMQ and publishes to both output-topic Kafka topic, and uses the custom Postgres plugin for publishing to postgres_queue , which is a table. middle-worker randomly sends messages back to the input-topic. ./example/external_data_consumer.py consumes from output-topic and logs to console.","title":"Advanced Example"},{"location":"advanced_example/#advanced-example","text":"Run the pre-built example from the root level of this repository with make run.example You will need docker , and docker-compose installed on your machine. Kafka, Redis, and Postgres are each spawned in their own Docker containers defined in docker-compose.yml . The example consists of the Volley workers, which are functions described below. ./example/external_data_producer.py publishes sample data to input-topic Kafka topic. ./example/input_worker.py consumes from input-topic and publishes to redis_queue , RSMQ queue. ./example/middle_worker.py consumes from redis_queue RSMQ and publishes to both output-topic Kafka topic, and uses the custom Postgres plugin for publishing to postgres_queue , which is a table. middle-worker randomly sends messages back to the input-topic. ./example/external_data_consumer.py consumes from output-topic and logs to console.","title":"Advanced Example"},{"location":"deadletterqueue/","text":"Dead Letter Queues \u00b6 Dead letter queues (DLQ) are special types of queues that are typically reserved for messages that have failed processing. If enabled, Volley will publish messages, which fail either serialization or schema validation, to a specified dead letter queue. Application can also electively send a message to a dead letter queue by providing it specifying the dead letter queue by name in the functions return statement. To configure a dead letter queue, provide it in queue configuration: config = { \"input-topic\" : { \"value\" : \"long.name.of.kafka.input.topic\" , \"profile\" : \"confluent\" , } \"my-dead-letter-queue\" : { \"value\" : \"long.name.of.kafka.DLQ.topic\" , \"profile\" : \"confluent-dlq\" , } } The confluent-dlq profile disables serialization and model handling. This routes the raw message from the input topic to the dead-letter-queue. Then specify the DLQ on Engine init: from volley.engine import Engine app = Engine ( app_name = \"my_app\" , input_queue = \"input-topic, output_queue = None , dead_letter_queue = \"my-dead-letter-queue\" # this enables the DLQ queue_config = config ) Messages that fail serialization or validation will be routed to the dead-letter-queue (and never reach your application). Unhandled exceptions within the user application are NOT handled by Volley's DLQ mechanism. Alternatively, your application can manually route a message to the dead-letter-queue. However, by default there is no serialization or data validation for the DLQ. This is because Volley DLQs are designed to be a close mirror of the input-queue and allow for ease of replaying of messages. Therefore, if you want to manually publish to a DLQ, you will need to manually convert your data to bytes. from pydantic import BaseModel class myInput ( BaseModel ): myData : int @app . stream_app def my_app ( msg : myInput ) -> Union [ List [ Tuple [ str , myInput ]], bool ]: \"\"\"function that manually routes unhandled exceptions to a DLQ\"\"\" try : rando = randint ( 0 , 1 ) 1 / rando except ZeroDivisionError : # send to DLQ as bytes output_message = msg . model_dump_json () . encode ( \"utf-8\" ) return [( \"my-dead-letter-queue\" , output_message )] # eat the message, mark incoming message as \"success\" return True Volley creates ERROR level logs whenever messages fail serialization or validation. There are also Prometheus metrics logged each time a message is published to any queue, including dead-letter-queues. It is recommended to create alerts on these metrics to help monitor this process. For example, the below Prometheus query would result in the total number of messages that have been produced to the dead letter topic over the prior 24h time window. increase ( messages_produced_count_total { destination = \" my-dead-letter-queue \"}[ 24h ] )","title":"Dead Letter Queues"},{"location":"deadletterqueue/#dead-letter-queues","text":"Dead letter queues (DLQ) are special types of queues that are typically reserved for messages that have failed processing. If enabled, Volley will publish messages, which fail either serialization or schema validation, to a specified dead letter queue. Application can also electively send a message to a dead letter queue by providing it specifying the dead letter queue by name in the functions return statement. To configure a dead letter queue, provide it in queue configuration: config = { \"input-topic\" : { \"value\" : \"long.name.of.kafka.input.topic\" , \"profile\" : \"confluent\" , } \"my-dead-letter-queue\" : { \"value\" : \"long.name.of.kafka.DLQ.topic\" , \"profile\" : \"confluent-dlq\" , } } The confluent-dlq profile disables serialization and model handling. This routes the raw message from the input topic to the dead-letter-queue. Then specify the DLQ on Engine init: from volley.engine import Engine app = Engine ( app_name = \"my_app\" , input_queue = \"input-topic, output_queue = None , dead_letter_queue = \"my-dead-letter-queue\" # this enables the DLQ queue_config = config ) Messages that fail serialization or validation will be routed to the dead-letter-queue (and never reach your application). Unhandled exceptions within the user application are NOT handled by Volley's DLQ mechanism. Alternatively, your application can manually route a message to the dead-letter-queue. However, by default there is no serialization or data validation for the DLQ. This is because Volley DLQs are designed to be a close mirror of the input-queue and allow for ease of replaying of messages. Therefore, if you want to manually publish to a DLQ, you will need to manually convert your data to bytes. from pydantic import BaseModel class myInput ( BaseModel ): myData : int @app . stream_app def my_app ( msg : myInput ) -> Union [ List [ Tuple [ str , myInput ]], bool ]: \"\"\"function that manually routes unhandled exceptions to a DLQ\"\"\" try : rando = randint ( 0 , 1 ) 1 / rando except ZeroDivisionError : # send to DLQ as bytes output_message = msg . model_dump_json () . encode ( \"utf-8\" ) return [( \"my-dead-letter-queue\" , output_message )] # eat the message, mark incoming message as \"success\" return True Volley creates ERROR level logs whenever messages fail serialization or validation. There are also Prometheus metrics logged each time a message is published to any queue, including dead-letter-queues. It is recommended to create alerts on these metrics to help monitor this process. For example, the below Prometheus query would result in the total number of messages that have been produced to the dead letter topic over the prior 24h time window. increase ( messages_produced_count_total { destination = \" my-dead-letter-queue \"}[ 24h ] )","title":"Dead Letter Queues"},{"location":"engine/","text":"Application \u00b6 The engine prepares a Python decorator that wraps a worker function to be run as a headless service. App Configuration \u00b6 All configuration is passed in and initialized via the Engine class from volley.engine import Engine app = Engine ( ... ) Initializes the Volley application and prepares the main decorator Attributes: Name Type Description app_name str Name of the application. Added as a label to all logged metrics input_queue str Name of the input queue. Corresponds to the name of one queue defined in queue_config or yaml_config_path output_queues List[str] List of queues the application needs to be able to publish to. Not required if the application does not produce anywhere! dead_letter_queue Optional[str] Points to the queue in configuration used as the dead letter queue. poll_interval_seconds float globally set time to to halt between polls on the consumer queue_config Union[List[volley.config.QueueConfig], Dict[str, Any]] either a list of QueueConfig (one for each queue) or a dictionary of queue configurations. Must provide one of queue_config or yaml_config_path yaml_config_path str path to a yaml config file. Must provide one of yaml_config_path or queue_config Exceptions: Type Description NameError input_queue, dead_letter_queue or output_queues reference a queue that does not exist in queue configuration Returns: Type Description Engine Instance of the engine with a prepared stream_app decorator __post_init__ ( self ) special \u00b6 Validates configuration and initializes queue configs Database connections are initialized within stream_app decorator Source code in volley/engine.py def __post_init__ ( self ) -> None : \"\"\"Validates configuration and initializes queue configs Database connections are initialized within stream_app decorator \"\"\" self . killer : GracefulKiller = GracefulKiller () if self . output_queues == []: logger . warning ( \"No output queues provided\" ) # queue config can come in three ways # 1. List[QueueConfig] , list of typed configurations, one for each queue # 2. dictionary, queueName: configurations # 3. yaml file if isinstance ( self . queue_config , list ): cfg = { x . name : x . custom_model_dump () for x in self . queue_config } elif isinstance ( self . queue_config , dict ): cfg = self . queue_config else : logger . info ( \"loading configuration from %s \" , self . yaml_config_path ) cfg = load_yaml ( file_path = self . yaml_config_path )[ \"queues\" ] # handle DLQ if self . dead_letter_queue is not None : # if provided by user, DLQ becomes a producer target # flag the queue using the DLQ profile if self . dead_letter_queue not in cfg : raise KeyError ( f \" { self . dead_letter_queue } not present in configuration\" ) else : self . output_queues . append ( self . dead_letter_queue ) else : logger . warning ( \"DLQ not provided. Application will crash on schema violations\" ) # cfg can contain configs for more queues that the app needs # filter out queues that are not required by app cfg = { k : v for k , v in cfg . items () if k in [ self . input_queue ] + self . output_queues } # validate input_queue, output_queues, and DLQ (optional) are valid configurations for q in [ self . input_queue ] + self . output_queues : if q not in cfg : raise KeyError ( f \"Queue ' { q } ' not found in configuration\" ) # tag queues with type (how app intends to use it) cfg [ self . input_queue ][ \"connection_type\" ] = ConnectionType . CONSUMER for qname in self . output_queues : cfg [ qname ][ \"connection_type\" ] = ConnectionType . PRODUCER # load profiles profiles : Dict [ str , Profile ] = construct_profiles ( cfg ) # create queue_map from profiles self . queue_map = construct_queue_map ( profiles , cfg ) logger . info ( \"Queues initialized: %s \" , list ( self . queue_map . keys ())) shutdown ( self ) \u00b6 graceful shutdown of all queue connections Source code in volley/engine.py def shutdown ( self ) -> None : \"\"\"graceful shutdown of all queue connections\"\"\" logger . info ( \"Shutting down %s , %s \" , self . app_name , self . queue_map [ self . input_queue ] . value ) for q_name in self . output_queues : out_queue = self . queue_map [ q_name ] logger . info ( \"Shutting down %s , %s \" , self . app_name , out_queue . value ) out_queue . producer_con . shutdown () logger . info ( \" %s , %s shutdown complete\" , self . app_name , q_name ) self . queue_map [ self . input_queue ] . consumer_con . shutdown () logger . info ( \"Shutdown %s complete\" , self . app_name ) stream_app ( self , func ) \u00b6 Main decorator for applications Source code in volley/engine.py def stream_app ( # noqa: C901 self , func : Callable [ ... , Union [ Awaitable [ Any ], List [ Tuple [ str , Any ]], List [ Tuple [ str , Any , Dict [ str , Any ]]], bool ]], ) -> Callable [ ... , None ]: \"\"\"Main decorator for applications\"\"\" _func = FuncEnvelope ( func ) @run_async @wraps ( func ) async def run_component () -> None : if self . metrics_port is not None : serve_metrics ( port = self . metrics_port ) # the component function is passed in as `func` # first setup the connections to the input and outputs queues that the component will need # we only want to set these up once, before the component is invoked # initialize connections to each queue, and schemas self . queue_map [ self . input_queue ] . connect ( con_type = ConnectionType . CONSUMER ) # we only want to connect to queues passed in to Engine() # there can be more queues than we need defined in the configuration yaml for out_q_name in self . output_queues : self . queue_map [ out_q_name ] . connect ( con_type = ConnectionType . PRODUCER ) # queue connections were setup above. now we can start to interact with the queues # alias for input connection readability input_con : Queue = self . queue_map [ self . input_queue ] # if asynchronous producer, give the consumer's \"on_success\" method to the producer for qname in self . output_queues : producer_con = self . queue_map [ qname ] . producer_con if producer_con . callback_delivery : producer_con . init_callbacks ( consumer = self . queue_map [ self . input_queue ] . consumer_con ) logger . info ( \"Starting Volley application: %s \" , self . app_name ) while not self . killer . kill_now : HEARTBEAT . inc () _start_time = time . time () # read message off the specified queue in_message : Optional [ QueueMessage ] = input_con . consumer_con . consume () if in_message is None : # if no messages, handle poll interval # TODO: this should be dynamic with some sort of backoff logger . debug ( \"No messages - sleeping POLL_INTERVAL= %s \" , self . poll_interval_seconds ) time . sleep ( self . poll_interval_seconds ) continue # typing for producing outputs : Union [ List [ Tuple [ str , Any ]], List [ Tuple [ str , Any , Dict [ str , Any ]]], bool ] = False data_model , consume_status = message_model_handler ( message = in_message . message , schema = input_con . data_model , model_handler = input_con . model_handler , serializer = input_con . serializer , ) if consume_status : # happy path MESSAGE_CONSUMED . labels ( volley_app = self . app_name , status = \"success\" ) . inc () elif self . dead_letter_queue is not None and self . dead_letter_queue in self . queue_map : outputs = [( self . dead_letter_queue , data_model )] MESSAGE_CONSUMED . labels ( volley_app = self . app_name , status = \"fail\" ) . inc () else : # things have gone wrong w/ the message and no DLQ configured MESSAGE_CONSUMED . labels ( volley_app = self . app_name , status = \"fail\" ) . inc () raise DLQNotConfiguredError ( f \"Deserializing { in_message . message } failed\" ) # component processing if not outputs : # this is happy path # if outputs have been assigned it means this message is destined for a DLQ _start_main = time . time () outputs = await run_worker_function ( app_name = self . app_name , f = _func , message = data_model , ctx = in_message . message_context , ) _fun_duration = time . time () - _start_main PROCESS_TIME . labels ( volley_app = self . app_name , process_name = \"component\" ) . observe ( _fun_duration ) delivery_report : DeliveryReport if isinstance ( outputs , builtins . bool ): # if func returns a bool, its just a bool and nothing more # there is no \"producer\" in this model. # Just mark the consumed message as either success or fail delivery_report = DeliveryReport ( status = outputs , asynchronous = False ) else : delivery_reports : List [ DeliveryReport ] = produce_handler ( outputs = outputs , queue_map = self . queue_map , app_name = self . app_name , input_name = input_con . name , message_context = in_message . message_context , ) delivery_report = delivery_success ( delivery_reports ) if delivery_report . status is True and not delivery_report . asynchronous : # asynchronous delivery reports are handled within the Producer's callback # synchronous delivery reports are handled here input_con . consumer_con . on_success ( message_context = in_message . message_context , ) elif not delivery_report . asynchronous : input_con . consumer_con . on_fail ( message_context = in_message . message_context , ) _duration = time . time () - _start_time PROCESS_TIME . labels ( volley_app = self . app_name , process_name = \"cycle\" ) . observe ( _duration ) if RUN_ONCE : # for testing purposes only - mock RUN_ONCE break self . shutdown () # used for unit testing as a means to access the wrapped component without the decorator run_component . __wrapped__ = func # type: ignore return run_component","title":"Application"},{"location":"engine/#application","text":"The engine prepares a Python decorator that wraps a worker function to be run as a headless service.","title":"Application"},{"location":"engine/#app-configuration","text":"All configuration is passed in and initialized via the Engine class from volley.engine import Engine app = Engine ( ... ) Initializes the Volley application and prepares the main decorator Attributes: Name Type Description app_name str Name of the application. Added as a label to all logged metrics input_queue str Name of the input queue. Corresponds to the name of one queue defined in queue_config or yaml_config_path output_queues List[str] List of queues the application needs to be able to publish to. Not required if the application does not produce anywhere! dead_letter_queue Optional[str] Points to the queue in configuration used as the dead letter queue. poll_interval_seconds float globally set time to to halt between polls on the consumer queue_config Union[List[volley.config.QueueConfig], Dict[str, Any]] either a list of QueueConfig (one for each queue) or a dictionary of queue configurations. Must provide one of queue_config or yaml_config_path yaml_config_path str path to a yaml config file. Must provide one of yaml_config_path or queue_config Exceptions: Type Description NameError input_queue, dead_letter_queue or output_queues reference a queue that does not exist in queue configuration Returns: Type Description Engine Instance of the engine with a prepared stream_app decorator","title":"App Configuration"},{"location":"engine/#volley.engine.Engine.__post_init__","text":"Validates configuration and initializes queue configs Database connections are initialized within stream_app decorator Source code in volley/engine.py def __post_init__ ( self ) -> None : \"\"\"Validates configuration and initializes queue configs Database connections are initialized within stream_app decorator \"\"\" self . killer : GracefulKiller = GracefulKiller () if self . output_queues == []: logger . warning ( \"No output queues provided\" ) # queue config can come in three ways # 1. List[QueueConfig] , list of typed configurations, one for each queue # 2. dictionary, queueName: configurations # 3. yaml file if isinstance ( self . queue_config , list ): cfg = { x . name : x . custom_model_dump () for x in self . queue_config } elif isinstance ( self . queue_config , dict ): cfg = self . queue_config else : logger . info ( \"loading configuration from %s \" , self . yaml_config_path ) cfg = load_yaml ( file_path = self . yaml_config_path )[ \"queues\" ] # handle DLQ if self . dead_letter_queue is not None : # if provided by user, DLQ becomes a producer target # flag the queue using the DLQ profile if self . dead_letter_queue not in cfg : raise KeyError ( f \" { self . dead_letter_queue } not present in configuration\" ) else : self . output_queues . append ( self . dead_letter_queue ) else : logger . warning ( \"DLQ not provided. Application will crash on schema violations\" ) # cfg can contain configs for more queues that the app needs # filter out queues that are not required by app cfg = { k : v for k , v in cfg . items () if k in [ self . input_queue ] + self . output_queues } # validate input_queue, output_queues, and DLQ (optional) are valid configurations for q in [ self . input_queue ] + self . output_queues : if q not in cfg : raise KeyError ( f \"Queue ' { q } ' not found in configuration\" ) # tag queues with type (how app intends to use it) cfg [ self . input_queue ][ \"connection_type\" ] = ConnectionType . CONSUMER for qname in self . output_queues : cfg [ qname ][ \"connection_type\" ] = ConnectionType . PRODUCER # load profiles profiles : Dict [ str , Profile ] = construct_profiles ( cfg ) # create queue_map from profiles self . queue_map = construct_queue_map ( profiles , cfg ) logger . info ( \"Queues initialized: %s \" , list ( self . queue_map . keys ()))","title":"__post_init__()"},{"location":"engine/#volley.engine.Engine.shutdown","text":"graceful shutdown of all queue connections Source code in volley/engine.py def shutdown ( self ) -> None : \"\"\"graceful shutdown of all queue connections\"\"\" logger . info ( \"Shutting down %s , %s \" , self . app_name , self . queue_map [ self . input_queue ] . value ) for q_name in self . output_queues : out_queue = self . queue_map [ q_name ] logger . info ( \"Shutting down %s , %s \" , self . app_name , out_queue . value ) out_queue . producer_con . shutdown () logger . info ( \" %s , %s shutdown complete\" , self . app_name , q_name ) self . queue_map [ self . input_queue ] . consumer_con . shutdown () logger . info ( \"Shutdown %s complete\" , self . app_name )","title":"shutdown()"},{"location":"engine/#volley.engine.Engine.stream_app","text":"Main decorator for applications Source code in volley/engine.py def stream_app ( # noqa: C901 self , func : Callable [ ... , Union [ Awaitable [ Any ], List [ Tuple [ str , Any ]], List [ Tuple [ str , Any , Dict [ str , Any ]]], bool ]], ) -> Callable [ ... , None ]: \"\"\"Main decorator for applications\"\"\" _func = FuncEnvelope ( func ) @run_async @wraps ( func ) async def run_component () -> None : if self . metrics_port is not None : serve_metrics ( port = self . metrics_port ) # the component function is passed in as `func` # first setup the connections to the input and outputs queues that the component will need # we only want to set these up once, before the component is invoked # initialize connections to each queue, and schemas self . queue_map [ self . input_queue ] . connect ( con_type = ConnectionType . CONSUMER ) # we only want to connect to queues passed in to Engine() # there can be more queues than we need defined in the configuration yaml for out_q_name in self . output_queues : self . queue_map [ out_q_name ] . connect ( con_type = ConnectionType . PRODUCER ) # queue connections were setup above. now we can start to interact with the queues # alias for input connection readability input_con : Queue = self . queue_map [ self . input_queue ] # if asynchronous producer, give the consumer's \"on_success\" method to the producer for qname in self . output_queues : producer_con = self . queue_map [ qname ] . producer_con if producer_con . callback_delivery : producer_con . init_callbacks ( consumer = self . queue_map [ self . input_queue ] . consumer_con ) logger . info ( \"Starting Volley application: %s \" , self . app_name ) while not self . killer . kill_now : HEARTBEAT . inc () _start_time = time . time () # read message off the specified queue in_message : Optional [ QueueMessage ] = input_con . consumer_con . consume () if in_message is None : # if no messages, handle poll interval # TODO: this should be dynamic with some sort of backoff logger . debug ( \"No messages - sleeping POLL_INTERVAL= %s \" , self . poll_interval_seconds ) time . sleep ( self . poll_interval_seconds ) continue # typing for producing outputs : Union [ List [ Tuple [ str , Any ]], List [ Tuple [ str , Any , Dict [ str , Any ]]], bool ] = False data_model , consume_status = message_model_handler ( message = in_message . message , schema = input_con . data_model , model_handler = input_con . model_handler , serializer = input_con . serializer , ) if consume_status : # happy path MESSAGE_CONSUMED . labels ( volley_app = self . app_name , status = \"success\" ) . inc () elif self . dead_letter_queue is not None and self . dead_letter_queue in self . queue_map : outputs = [( self . dead_letter_queue , data_model )] MESSAGE_CONSUMED . labels ( volley_app = self . app_name , status = \"fail\" ) . inc () else : # things have gone wrong w/ the message and no DLQ configured MESSAGE_CONSUMED . labels ( volley_app = self . app_name , status = \"fail\" ) . inc () raise DLQNotConfiguredError ( f \"Deserializing { in_message . message } failed\" ) # component processing if not outputs : # this is happy path # if outputs have been assigned it means this message is destined for a DLQ _start_main = time . time () outputs = await run_worker_function ( app_name = self . app_name , f = _func , message = data_model , ctx = in_message . message_context , ) _fun_duration = time . time () - _start_main PROCESS_TIME . labels ( volley_app = self . app_name , process_name = \"component\" ) . observe ( _fun_duration ) delivery_report : DeliveryReport if isinstance ( outputs , builtins . bool ): # if func returns a bool, its just a bool and nothing more # there is no \"producer\" in this model. # Just mark the consumed message as either success or fail delivery_report = DeliveryReport ( status = outputs , asynchronous = False ) else : delivery_reports : List [ DeliveryReport ] = produce_handler ( outputs = outputs , queue_map = self . queue_map , app_name = self . app_name , input_name = input_con . name , message_context = in_message . message_context , ) delivery_report = delivery_success ( delivery_reports ) if delivery_report . status is True and not delivery_report . asynchronous : # asynchronous delivery reports are handled within the Producer's callback # synchronous delivery reports are handled here input_con . consumer_con . on_success ( message_context = in_message . message_context , ) elif not delivery_report . asynchronous : input_con . consumer_con . on_fail ( message_context = in_message . message_context , ) _duration = time . time () - _start_time PROCESS_TIME . labels ( volley_app = self . app_name , process_name = \"cycle\" ) . observe ( _duration ) if RUN_ONCE : # for testing purposes only - mock RUN_ONCE break self . shutdown () # used for unit testing as a means to access the wrapped component without the decorator run_component . __wrapped__ = func # type: ignore return run_component","title":"stream_app()"},{"location":"example/","text":"Getting Started \u00b6 This is a basic example of a common pattern, an application that consumes from a kafka topic and publishes to another kafka topic. The example application receives a list of floats in a message from the input Kafka topic and publishes the maximum value from that list to the output Kafka Topic. Volley treats Kafka topics like queues, so the term \"queue\" and \"topic\" will be used interchangeably. 1. Define data models \u00b6 Let's define two Pydantic models; one for the input and one for the output Kafka topic. We'll build the application such that it sends/receives these two objects from Volley. # my_models.py from pydantic import BaseModel class InputMessage ( BaseModel ): \"\"\"validate the incoming data\"\"\" list_of_values : list [ float ] class OutputMessage ( BaseModel ): \"\"\"validate the outgoing data\"\"\" max_value : float 2. Queue configuration \u00b6 Let's create a dictionary of configurations for each Kafka topic. Each key in the dictionary contains the configurations for that topic. These configurations define the relationship between your application and the queue, and generally determine how your application will consume/produce to the queue, and how it wants to parse the data to/from the queue. See also queue configuration for more detailed explanation of queues First, let's define the configuration for the \"input-queue\". from my_models import InputMessage # app.py input_topic_cfg = { \"value\" : \"incoming.kafka.topic\" , \"consumer\" : \"volley.connectors.ConfluentKafkaConsumer\" , \"serializer\" : \"volley.serializers.OrJsonSerialization\" , \"model_handler\" : \"volley.models.PydanticModelHandler\" , \"data_model\" : InputMessage , \"config\" : { \"group.id\" : \"my-consumer-group\" , \"bootstrap.servers\" : \"kafka:9092\" , } } input-topic is what we'll use to refer to this topic in our application. value is actual name of the topic on the broker. consumer refers to the class which defines how we will consume a message from this topic. serializer is what we'll use to serialize the data from bytes data_model is the class that define schemas for data on the topics model_handler tells Volley how to create an instance of your data_model with the data returned from the serializer . config are value to pass directly to the connector constructors. In this case, we are using Confluent's consumer, so these values directly to the config values supported by librdkafka Volley has built in support for consumer (and producer ) serializer , data_model and model_handler . These values are all dot paths to classes. You can extend Volley with your own classes for all of this functionality. Next, let's do the same for the \"output-topic\", but instead of defining all of the config attributions, let's use a Profile . Profiles are pre-set groups of configuration to make it easier to decide how to handle the interaction with the queue. For the \"output-topic\", let's use the confluent profile , which sets the consumer, producer, serializer, model_handler, and data_model for us. However, instead of using the data_model provided by the confluent Profile, let's override it with the Pydantic data_model we defined earlier. Profiles (currently) do not include config attributes, so we still need to set those. # app.py from my_models import OutputMessage output_topic_cfg = { \"value\" : \"outgoing.kafka.topic\" , \"profile\" : \"confluent\" , \"data_model\" : OutputMessage , \"config\" : { \"bootstrap.servers\" : \"kafka:9092\" } } 3. Initialize the Engine \u00b6 from volley import Engine queue_config = { \"input-topic\" : input_topic_cfg , \"output-topic\" : output_topic_cfg , } app = Engine ( app_name = \"my_volley_app\" , input_queue = \"input-topic\" , output_queues = [ \"output-topic\" ], queue_config = queue_config , ) When we instantiate Engine() , we specify which of the queues we want to treat as inputs and which are outputs. Notice we can have any number of outputs, but only one input. However, the input queue can be a target in the output_queues. If we do that, we just need to make sure the configuration provides a producer either explicitly or through a Profile. app_name translates to labels used in logging and metrics . 4. Decorate the function \u00b6 Apply the Engine's decorator to your function. Volley from typing import List , Tuple from .my_models import InputMessage , OutputMessage @app . stream_app def my_app ( msg : InputMessage ) -> List [ Tuple [ str , OutputMessage ]]: max_value = max ( msg . list_of_values ) output = OutputMessage ( max_value = max_value ) return [( \"output-topic\" , output )] if __name__ == __main__ : my_app () When run, the function will receive a message of the type which was defined in the data_model in queue_config . You produce messages by returning a list of tuples from your function. List [ Tuple [ str , Any , Optional [ Dict [ str Any ]]]] Each element in the list is a message destined for the queue specified by the first element in the tuple. The second tuple element is the message itself. The third tuple element is optional runtime configurations for your producer. For the Confluent producer, these might be a partition key or a callback. They are passed to the producer's produce() function as expanded key word arguments. [(output_queue_name, message_object, optional_runtime_configurations)] Your application does not need to produce anything! Simply return True if you want Volley to mark the incoming message as a success with the consuming input queue, and False if you want to mark it as failed. With RSMQ, True means the incoming message will be deleted from the input queue and False results in the incoming message being returned to the input queue. See the consumer's on_success and on_fail implementations for specifics. 5. Run application \u00b6 Run it! python -m app","title":"Getting Started"},{"location":"example/#getting-started","text":"This is a basic example of a common pattern, an application that consumes from a kafka topic and publishes to another kafka topic. The example application receives a list of floats in a message from the input Kafka topic and publishes the maximum value from that list to the output Kafka Topic. Volley treats Kafka topics like queues, so the term \"queue\" and \"topic\" will be used interchangeably.","title":"Getting Started"},{"location":"example/#1-define-data-models","text":"Let's define two Pydantic models; one for the input and one for the output Kafka topic. We'll build the application such that it sends/receives these two objects from Volley. # my_models.py from pydantic import BaseModel class InputMessage ( BaseModel ): \"\"\"validate the incoming data\"\"\" list_of_values : list [ float ] class OutputMessage ( BaseModel ): \"\"\"validate the outgoing data\"\"\" max_value : float","title":"1. Define data models"},{"location":"example/#2-queue-configuration","text":"Let's create a dictionary of configurations for each Kafka topic. Each key in the dictionary contains the configurations for that topic. These configurations define the relationship between your application and the queue, and generally determine how your application will consume/produce to the queue, and how it wants to parse the data to/from the queue. See also queue configuration for more detailed explanation of queues First, let's define the configuration for the \"input-queue\". from my_models import InputMessage # app.py input_topic_cfg = { \"value\" : \"incoming.kafka.topic\" , \"consumer\" : \"volley.connectors.ConfluentKafkaConsumer\" , \"serializer\" : \"volley.serializers.OrJsonSerialization\" , \"model_handler\" : \"volley.models.PydanticModelHandler\" , \"data_model\" : InputMessage , \"config\" : { \"group.id\" : \"my-consumer-group\" , \"bootstrap.servers\" : \"kafka:9092\" , } } input-topic is what we'll use to refer to this topic in our application. value is actual name of the topic on the broker. consumer refers to the class which defines how we will consume a message from this topic. serializer is what we'll use to serialize the data from bytes data_model is the class that define schemas for data on the topics model_handler tells Volley how to create an instance of your data_model with the data returned from the serializer . config are value to pass directly to the connector constructors. In this case, we are using Confluent's consumer, so these values directly to the config values supported by librdkafka Volley has built in support for consumer (and producer ) serializer , data_model and model_handler . These values are all dot paths to classes. You can extend Volley with your own classes for all of this functionality. Next, let's do the same for the \"output-topic\", but instead of defining all of the config attributions, let's use a Profile . Profiles are pre-set groups of configuration to make it easier to decide how to handle the interaction with the queue. For the \"output-topic\", let's use the confluent profile , which sets the consumer, producer, serializer, model_handler, and data_model for us. However, instead of using the data_model provided by the confluent Profile, let's override it with the Pydantic data_model we defined earlier. Profiles (currently) do not include config attributes, so we still need to set those. # app.py from my_models import OutputMessage output_topic_cfg = { \"value\" : \"outgoing.kafka.topic\" , \"profile\" : \"confluent\" , \"data_model\" : OutputMessage , \"config\" : { \"bootstrap.servers\" : \"kafka:9092\" } }","title":"2. Queue configuration"},{"location":"example/#3-initialize-the-engine","text":"from volley import Engine queue_config = { \"input-topic\" : input_topic_cfg , \"output-topic\" : output_topic_cfg , } app = Engine ( app_name = \"my_volley_app\" , input_queue = \"input-topic\" , output_queues = [ \"output-topic\" ], queue_config = queue_config , ) When we instantiate Engine() , we specify which of the queues we want to treat as inputs and which are outputs. Notice we can have any number of outputs, but only one input. However, the input queue can be a target in the output_queues. If we do that, we just need to make sure the configuration provides a producer either explicitly or through a Profile. app_name translates to labels used in logging and metrics .","title":"3. Initialize the Engine"},{"location":"example/#4-decorate-the-function","text":"Apply the Engine's decorator to your function. Volley from typing import List , Tuple from .my_models import InputMessage , OutputMessage @app . stream_app def my_app ( msg : InputMessage ) -> List [ Tuple [ str , OutputMessage ]]: max_value = max ( msg . list_of_values ) output = OutputMessage ( max_value = max_value ) return [( \"output-topic\" , output )] if __name__ == __main__ : my_app () When run, the function will receive a message of the type which was defined in the data_model in queue_config . You produce messages by returning a list of tuples from your function. List [ Tuple [ str , Any , Optional [ Dict [ str Any ]]]] Each element in the list is a message destined for the queue specified by the first element in the tuple. The second tuple element is the message itself. The third tuple element is optional runtime configurations for your producer. For the Confluent producer, these might be a partition key or a callback. They are passed to the producer's produce() function as expanded key word arguments. [(output_queue_name, message_object, optional_runtime_configurations)] Your application does not need to produce anything! Simply return True if you want Volley to mark the incoming message as a success with the consuming input queue, and False if you want to mark it as failed. With RSMQ, True means the incoming message will be deleted from the input queue and False results in the incoming message being returned to the input queue. See the consumer's on_success and on_fail implementations for specifics.","title":"4. Decorate the function"},{"location":"example/#5-run-application","text":"Run it! python -m app","title":"5. Run application"},{"location":"extending/","text":"Extending Volley \u00b6 Volley can be extended by developing additional connectors, serializers, and model handlers. Each of these have base classes that can be inherited into your own implementations, then registered in the Engine() configuration to be used with your application. Briefly: Connectors handle consuming and producing data to/from a data store Serializers convert bytes to a primitive python object Model handlers convert primitive python object to user defined data models Each of these can be extended with custom and community contributed plugins.","title":"Overview"},{"location":"extending/#extending-volley","text":"Volley can be extended by developing additional connectors, serializers, and model handlers. Each of these have base classes that can be inherited into your own implementations, then registered in the Engine() configuration to be used with your application. Briefly: Connectors handle consuming and producing data to/from a data store Serializers convert bytes to a primitive python object Model handlers convert primitive python object to user defined data models Each of these can be extended with custom and community contributed plugins.","title":"Extending Volley"},{"location":"metrics/","text":"Metrics \u00b6 Volley exposes Prometheus metrics on an http server. The metrics serving port can be configured or disabled completely. Prometheus Multiprocess Mode is also supported, just set PROMETHEUS_MULTIPROC_DIR to an appropriate value. See Prometheus docs for more details. Expose metrics on port 8081. from volley import Engine app = Engine ( ... metrics_port = 8081 ) To disable: from volley import Engine app = Engine ( ... metrics_port = None # disabled ) All metrics contain the label volley_app which is directly tied to the app_name parameter passed in when initializing volley.Engine() . Below are descriptions of each of the metrics produced by Volley. messages_consumed_count \u00b6 Type: Counter increments each time a message is consumed by the worker. Labels: status : success|fail . If the worker consumes a message, but fails the corresponding produce operation, the message gets marked as a fail . Otherwise, it is a success . messages_produced_count \u00b6 Type: Counter increments each time a message is produced Labels: source : name of the queue the worker consumed from. destination : name of the queue the message was produced to. process_time_seconds \u00b6 Type: Summary observed amount of time various processes take to run Labels: process_name : name of the process that is tracked Values: component : time associated with the processing time for the function that Volley wraps. This is isolated to the logic in the user's function. cycle : one full cycle of consume message, serialize, schema validation, component processing, and publishing to all outputs. component is a subset of cycle data_model_process_seconds \u00b6 Type: Summary observed time for serialization/deserialization and data model construct/deconstruct Labels: process : the observed process Values: deserialize , serialize , construct , deconstruct redis_process_time_seconds \u00b6 Type: Summary similar to process_time_seconds but is isolated to the RSMQ connector. Labels: operation : name of the operation. Values read : time to read a message from the queue delete : time to delete a message from the queue write : time to add a message to the queue heartbeats \u00b6 Type: Counter Counter is incremented at the start of each poll cycle. It is incremented more frequently when messages are consumed and at the rate of the volley.Engine poll_interval when no messages are available to be consumed. Labels: There are no labels on this metric. Applications can export their own metrics as well. Examples in the Prometheus official python client are a great place to start. The Volley exporter will collect these metrics and expose them to be scraped by a Prometheus server. To serve multiprocess metrics, disable Volley's metrics server and implement the Multiprocess collector according to the official python client docs . volley_app_completions \u00b6 Type: Counter Counter is incremented each time an application attempts to process a message. The labels depend on the outcome the application wrapped by volley.Engine.stream_app . The labels on the metric indicate the outcome of the application cycle. Labels: status : Values success : application processed a message without raising an exception. failure : application raised an exception to Volley.","title":"Metrics"},{"location":"metrics/#metrics","text":"Volley exposes Prometheus metrics on an http server. The metrics serving port can be configured or disabled completely. Prometheus Multiprocess Mode is also supported, just set PROMETHEUS_MULTIPROC_DIR to an appropriate value. See Prometheus docs for more details. Expose metrics on port 8081. from volley import Engine app = Engine ( ... metrics_port = 8081 ) To disable: from volley import Engine app = Engine ( ... metrics_port = None # disabled ) All metrics contain the label volley_app which is directly tied to the app_name parameter passed in when initializing volley.Engine() . Below are descriptions of each of the metrics produced by Volley.","title":"Metrics"},{"location":"metrics/#messages_consumed_count","text":"Type: Counter increments each time a message is consumed by the worker. Labels: status : success|fail . If the worker consumes a message, but fails the corresponding produce operation, the message gets marked as a fail . Otherwise, it is a success .","title":"messages_consumed_count"},{"location":"metrics/#messages_produced_count","text":"Type: Counter increments each time a message is produced Labels: source : name of the queue the worker consumed from. destination : name of the queue the message was produced to.","title":"messages_produced_count"},{"location":"metrics/#process_time_seconds","text":"Type: Summary observed amount of time various processes take to run Labels: process_name : name of the process that is tracked Values: component : time associated with the processing time for the function that Volley wraps. This is isolated to the logic in the user's function. cycle : one full cycle of consume message, serialize, schema validation, component processing, and publishing to all outputs. component is a subset of cycle","title":"process_time_seconds"},{"location":"metrics/#data_model_process_seconds","text":"Type: Summary observed time for serialization/deserialization and data model construct/deconstruct Labels: process : the observed process Values: deserialize , serialize , construct , deconstruct","title":"data_model_process_seconds"},{"location":"metrics/#redis_process_time_seconds","text":"Type: Summary similar to process_time_seconds but is isolated to the RSMQ connector. Labels: operation : name of the operation. Values read : time to read a message from the queue delete : time to delete a message from the queue write : time to add a message to the queue","title":"redis_process_time_seconds"},{"location":"metrics/#heartbeats","text":"Type: Counter Counter is incremented at the start of each poll cycle. It is incremented more frequently when messages are consumed and at the rate of the volley.Engine poll_interval when no messages are available to be consumed. Labels: There are no labels on this metric. Applications can export their own metrics as well. Examples in the Prometheus official python client are a great place to start. The Volley exporter will collect these metrics and expose them to be scraped by a Prometheus server. To serve multiprocess metrics, disable Volley's metrics server and implement the Multiprocess collector according to the official python client docs .","title":"heartbeats"},{"location":"metrics/#volley_app_completions","text":"Type: Counter Counter is incremented each time an application attempts to process a message. The labels depend on the outcome the application wrapped by volley.Engine.stream_app . The labels on the metric indicate the outcome of the application cycle. Labels: status : Values success : application processed a message without raising an exception. failure : application raised an exception to Volley.","title":"volley_app_completions"},{"location":"profiles/","text":"Profiles \u00b6 Overview \u00b6 Profiles are pre-defined sets of Volley configurations and can be partially or completely overridden. For example, define the confluent profile in the queue configuration config = { \"my-input-queue\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , } } or the RSMQ profile config = { \"my-input-queue\" : { \"profile\" : \"rsmq\" , \"value\" : \"my.redis.queue.name\" , } } Then initialize the application. from volley import Engine app = Engine ( input_queue = \"my-input-queue, queue_config = config ) Profiles define the following: consumer : (str) - dot path to the concrete implementation of the base Consumer . Consumers define how Volley should consume a message from a queue and mark a message as successfully read and processed. producer : (str) - dot path to the concrete implementation of the base Producer . Defines how Volley should produce a message to a queue. serializer : (str) - dot path to the concrete implementation of the base BaseSerialization . Defines how to turn raw bytes into a primitive python object. data_model : (str) - dot path to a user provided data model. Every model needs a model handler. model_handler : (str) - dot path to the concrete implementation of BaseModelHandler . Defines how Volley should turn serialized data into a user provided data model. Volley has built-in support for Pydantic models via volley.models.PydanticModelHandler and can be extended with custom model handlers. Usage \u00b6 Specify a profile from the list of supported profiles in the initialization configuration. config = { \"my-input-queue\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , } } If you wish to override any of of the configuration values from the \"confluent\" profile, just specify them. # /path/to/myAppDataModels.py from pydantic import BaseModel class myModel ( BaseModel ): my_str : str config = { \"my-input-queue\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , \"data_model\" : \"path.to.myAppDataModels.myModel\" } } Or use MessagePack for serialization: config = { \"my-input-queue\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , \"data_model\" : \"path.to.myAppDataModels.myModel\" , \"serializer\" : \"volley.serializers.msgpack_serializer.MsgPackSerialization\" , } } User Defined Configuration \u00b6 Profiles can be partially or completely overridden and are not explicitly required. If you do not provide a value for profile , you will need to provide valid configuration values for each of consumer , producer , serializer , model_handler , and data_model . These could be dot paths to your own custom implementations, or configurations that already exist in Volley. For example: from volley import Engine config = { \"my-input-queue\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , } } app = Engine ( input_queue = \"my-input-queue, queue_config = config ) Is equivalent to: config = { \"my-input-queue\" : { \"value\" : \"my.kafka.topic\" , \"consumer\" : \"volley.connectors.confluent.ConfluentKafkaConsumer\" , \"producer\" : \"volley.connectors.confluent.ConfluentKafkaProducer\" , \"model_handler\" : \"volley.models.PydanticModelHandler\" , \"data_model\" : \"volley.data_models.GenericMessage\" , \"serializer\" : \"volley.serializers.orjson_serializer.OrJsonSerialization\" , } } Refer to Extending Volley for instructions on writing your own Connectors , Serializers , and Model Handlers . All of the attributed provided by a profile can be overridden with existing or custom implementations. Supported Profiles \u00b6 The following is the complete list of the built-in profiles supported by Volley. Use them with: \"profile\": \"<name>\" in configuration. confluent \u00b6 The default confluent profile is most commonly used for applications working with Confluent Kafka brokers. It heavily relies on librdkafka and follows at-least-once delivery semantics by default. Consumed message offsets are auto-committed back to the Kafka broker. Messages are consumed from the Kafka broker as bytes , and serialized using orjson , and constructed into a generic Pydantic model. Many uses will provide their own value for data_model rather than using a generic Pydantic model. key value link consumer volley.connectors.confluent.ConfluentKafkaConsumer docs producer volley.connectors.confluent.ConfluentKafkaProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticModelHandler docs serializer volley.serializers.orjson_serializer.OrJsonSerialization docs confluent-batch-json \u00b6 Consumes a configurable number of messages from a Kafka topic. The application wrapped Engine.stream_app will receive a list of messages constructed in to the data model provided to the queue config. key value link consumer volley.connectors.confluent.BatchJsonConfluentConsumer docs producer volley.connectors.confluent.ConfluentKafkaProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticListParser docs serializer volley.serializers.orjson_serializer.OrJsonSerialization docs confluent-pydantic \u00b6 Very similar to the confluent profile. This profile uses Pydantic's default serializer mechanism to convert bytes to the Pydantic model. key value link consumer volley.connectors.confluent.ConfluentKafkaConsumer docs producer volley.connectors.confluent.ConfluentKafkaProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticParserModelHandler docs serializer None confluent-orjson-pydantic \u00b6 key value link consumer volley.connectors.confluent.ConfluentKafkaConsumer docs producer volley.connectors.confluent.ConfluentKafkaProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticModelHandler docs serializer volley.serializers.orjson_serializer.OrJsonSerialization docs confluent-msgpack-pydantic \u00b6 Parses a message as bytes from the Kafka broker. Serializes using MessagePack then constructs a Pydantic Model. key value link consumer volley.connectors.confluent.ConfluentKafkaConsumer docs producer volley.connectors.confluent.ConfluentKafkaProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticModelHandler docs serializer volley.serializers.msgpack_serializer.MsgPackSerialization docs rsmq \u00b6 The default Profile for interacting with pyRSMQ. Consumes a message from a Redis Simple Message Queue as bytes . Serializes with orjson and constructs a generic Pydantic model. The consumer deletes the consumed message once it is successfully processed. Messages that fail to parse or process are placed back on the originating queue, either explicitly by the consumer or by the queue itself after the visibility timeout expires. key value link consumer volley.connectors.rsmq.RSMQConsumer docs producer volley.connectors.rsmq.RSMQProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticModelHandler docs serializer volley.serializers.orjson_serializer.OrJsonSerialization docs rsmq-pydantic \u00b6 key value link consumer volley.connectors.rsmq.RSMQConsumer docs producer volley.connectors.rsmq.RSMQProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticParserModelHandler docs serializer None rsmq-orjson-pydantic \u00b6 key value link consumer volley.connectors.rsmq.RSMQConsumer docs producer volley.connectors.rsmq.RSMQProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticModelHandler docs serializer volley.serializers.orjson_serializer.OrJsonSerialization docs rsmq-msgpack-pydantic \u00b6 key value link consumer volley.connectors.rsmq.RSMQConsumer docs producer volley.connectors.rsmq.RSMQProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticModelHandler docs serializer volley.serializers.msgpack_serializer.MsgPackSerialization docs confluent-dlq \u00b6 Dead-letter-queue configuration to Confluent Kafka. Does not serialize or construct a data model for data consumed or produced. Generally only uses as a producer. key value link consumer volley.connectors.confluent.ConfluentKafkaConsumer docs producer volley.connectors.confluent.ConfluentKafkaProducer docs serializer None data_model None model_handler None rsmq-dlq \u00b6 Dead-letter-queue configuration to pyRSMQ. Does not serialize or construct a data model for data consumed or produced. Generally only uses as a producer. key value link consumer volley.connectors.rsmq.RSMQConsumer docs producer volley.connectors.rsmq.RSMQProducer docs serializer None data_model None model_handler None","title":"Profiles"},{"location":"profiles/#profiles","text":"","title":"Profiles"},{"location":"profiles/#overview","text":"Profiles are pre-defined sets of Volley configurations and can be partially or completely overridden. For example, define the confluent profile in the queue configuration config = { \"my-input-queue\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , } } or the RSMQ profile config = { \"my-input-queue\" : { \"profile\" : \"rsmq\" , \"value\" : \"my.redis.queue.name\" , } } Then initialize the application. from volley import Engine app = Engine ( input_queue = \"my-input-queue, queue_config = config ) Profiles define the following: consumer : (str) - dot path to the concrete implementation of the base Consumer . Consumers define how Volley should consume a message from a queue and mark a message as successfully read and processed. producer : (str) - dot path to the concrete implementation of the base Producer . Defines how Volley should produce a message to a queue. serializer : (str) - dot path to the concrete implementation of the base BaseSerialization . Defines how to turn raw bytes into a primitive python object. data_model : (str) - dot path to a user provided data model. Every model needs a model handler. model_handler : (str) - dot path to the concrete implementation of BaseModelHandler . Defines how Volley should turn serialized data into a user provided data model. Volley has built-in support for Pydantic models via volley.models.PydanticModelHandler and can be extended with custom model handlers.","title":"Overview"},{"location":"profiles/#usage","text":"Specify a profile from the list of supported profiles in the initialization configuration. config = { \"my-input-queue\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , } } If you wish to override any of of the configuration values from the \"confluent\" profile, just specify them. # /path/to/myAppDataModels.py from pydantic import BaseModel class myModel ( BaseModel ): my_str : str config = { \"my-input-queue\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , \"data_model\" : \"path.to.myAppDataModels.myModel\" } } Or use MessagePack for serialization: config = { \"my-input-queue\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , \"data_model\" : \"path.to.myAppDataModels.myModel\" , \"serializer\" : \"volley.serializers.msgpack_serializer.MsgPackSerialization\" , } }","title":"Usage"},{"location":"profiles/#user-defined-configuration","text":"Profiles can be partially or completely overridden and are not explicitly required. If you do not provide a value for profile , you will need to provide valid configuration values for each of consumer , producer , serializer , model_handler , and data_model . These could be dot paths to your own custom implementations, or configurations that already exist in Volley. For example: from volley import Engine config = { \"my-input-queue\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , } } app = Engine ( input_queue = \"my-input-queue, queue_config = config ) Is equivalent to: config = { \"my-input-queue\" : { \"value\" : \"my.kafka.topic\" , \"consumer\" : \"volley.connectors.confluent.ConfluentKafkaConsumer\" , \"producer\" : \"volley.connectors.confluent.ConfluentKafkaProducer\" , \"model_handler\" : \"volley.models.PydanticModelHandler\" , \"data_model\" : \"volley.data_models.GenericMessage\" , \"serializer\" : \"volley.serializers.orjson_serializer.OrJsonSerialization\" , } } Refer to Extending Volley for instructions on writing your own Connectors , Serializers , and Model Handlers . All of the attributed provided by a profile can be overridden with existing or custom implementations.","title":"User Defined Configuration"},{"location":"profiles/#supported-profiles","text":"The following is the complete list of the built-in profiles supported by Volley. Use them with: \"profile\": \"<name>\" in configuration.","title":"Supported Profiles"},{"location":"profiles/#confluent","text":"The default confluent profile is most commonly used for applications working with Confluent Kafka brokers. It heavily relies on librdkafka and follows at-least-once delivery semantics by default. Consumed message offsets are auto-committed back to the Kafka broker. Messages are consumed from the Kafka broker as bytes , and serialized using orjson , and constructed into a generic Pydantic model. Many uses will provide their own value for data_model rather than using a generic Pydantic model. key value link consumer volley.connectors.confluent.ConfluentKafkaConsumer docs producer volley.connectors.confluent.ConfluentKafkaProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticModelHandler docs serializer volley.serializers.orjson_serializer.OrJsonSerialization docs","title":"confluent"},{"location":"profiles/#confluent-batch-json","text":"Consumes a configurable number of messages from a Kafka topic. The application wrapped Engine.stream_app will receive a list of messages constructed in to the data model provided to the queue config. key value link consumer volley.connectors.confluent.BatchJsonConfluentConsumer docs producer volley.connectors.confluent.ConfluentKafkaProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticListParser docs serializer volley.serializers.orjson_serializer.OrJsonSerialization docs","title":"confluent-batch-json"},{"location":"profiles/#confluent-pydantic","text":"Very similar to the confluent profile. This profile uses Pydantic's default serializer mechanism to convert bytes to the Pydantic model. key value link consumer volley.connectors.confluent.ConfluentKafkaConsumer docs producer volley.connectors.confluent.ConfluentKafkaProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticParserModelHandler docs serializer None","title":"confluent-pydantic"},{"location":"profiles/#confluent-orjson-pydantic","text":"key value link consumer volley.connectors.confluent.ConfluentKafkaConsumer docs producer volley.connectors.confluent.ConfluentKafkaProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticModelHandler docs serializer volley.serializers.orjson_serializer.OrJsonSerialization docs","title":"confluent-orjson-pydantic"},{"location":"profiles/#confluent-msgpack-pydantic","text":"Parses a message as bytes from the Kafka broker. Serializes using MessagePack then constructs a Pydantic Model. key value link consumer volley.connectors.confluent.ConfluentKafkaConsumer docs producer volley.connectors.confluent.ConfluentKafkaProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticModelHandler docs serializer volley.serializers.msgpack_serializer.MsgPackSerialization docs","title":"confluent-msgpack-pydantic"},{"location":"profiles/#rsmq","text":"The default Profile for interacting with pyRSMQ. Consumes a message from a Redis Simple Message Queue as bytes . Serializes with orjson and constructs a generic Pydantic model. The consumer deletes the consumed message once it is successfully processed. Messages that fail to parse or process are placed back on the originating queue, either explicitly by the consumer or by the queue itself after the visibility timeout expires. key value link consumer volley.connectors.rsmq.RSMQConsumer docs producer volley.connectors.rsmq.RSMQProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticModelHandler docs serializer volley.serializers.orjson_serializer.OrJsonSerialization docs","title":"rsmq"},{"location":"profiles/#rsmq-pydantic","text":"key value link consumer volley.connectors.rsmq.RSMQConsumer docs producer volley.connectors.rsmq.RSMQProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticParserModelHandler docs serializer None","title":"rsmq-pydantic"},{"location":"profiles/#rsmq-orjson-pydantic","text":"key value link consumer volley.connectors.rsmq.RSMQConsumer docs producer volley.connectors.rsmq.RSMQProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticModelHandler docs serializer volley.serializers.orjson_serializer.OrJsonSerialization docs","title":"rsmq-orjson-pydantic"},{"location":"profiles/#rsmq-msgpack-pydantic","text":"key value link consumer volley.connectors.rsmq.RSMQConsumer docs producer volley.connectors.rsmq.RSMQProducer docs data_model volley.data_models.GenericMessage docs model_handler volley.models.PydanticModelHandler docs serializer volley.serializers.msgpack_serializer.MsgPackSerialization docs","title":"rsmq-msgpack-pydantic"},{"location":"profiles/#confluent-dlq","text":"Dead-letter-queue configuration to Confluent Kafka. Does not serialize or construct a data model for data consumed or produced. Generally only uses as a producer. key value link consumer volley.connectors.confluent.ConfluentKafkaConsumer docs producer volley.connectors.confluent.ConfluentKafkaProducer docs serializer None data_model None model_handler None","title":"confluent-dlq"},{"location":"profiles/#rsmq-dlq","text":"Dead-letter-queue configuration to pyRSMQ. Does not serialize or construct a data model for data consumed or produced. Generally only uses as a producer. key value link consumer volley.connectors.rsmq.RSMQConsumer docs producer volley.connectors.rsmq.RSMQProducer docs serializer None data_model None model_handler None","title":"rsmq-dlq"},{"location":"queue_config/","text":"Configuration \u00b6 Volley's configuration centers around queues. The configurations define how you application will interact with and how Volley parses data being received/sent to each queue. Queues have names, data models, model handlers, serializers, and connectors. It is recommended to define Volley's configuration by passing a dictionary directly the Engine initializer. Queue Configuration \u00b6 The queue configuration object is a dictionary passed into the Volley engine at initialization. It can be defined either via a list[QueueConfig] , a dict , or a .yaml file. List[QueueConfig] \u00b6 Represents the configuration for a single queue. Provides the application with necessary configuration for interacting with the specified queue. Attributes: Name Type Description name str Alias for a particular queue. value str The system name for a queue. For example, the name of a Kafka topic (prd.my.long.kafka.topic.name) or name of a RSMQ queue. profile Optional[str] Either kafka|rsmq . Pertains to the type of connector required to produce and consume from the queue. If not provided, must provide values for ALL of consumer, producer, serializer, model_handler . data_model Optional[str] Defaults to volley.data_models.GenericMessage . Path to the Pydantic model used for data validation. When default is used, Volley will only validate that messages can be successfully converted to a Pydantic model or dictionary. serializer Optional[str] Defaults to volley.serializers.OrJsonSerializer . Path to the serializer. producer Optional[str] Used for providing a custom producer connector. Overrides the producer pertaining to that provided in type . Provide the dot path to the producer class. e.g. for Kafka,defaults to volley.connectors.kafka.KafkaProducer ; cf. Extending Connectors . consumer Optional[str] Used for providing a custom consumer connector. Overrides the consumer pertaining to that provided in type . Provide the dot path to the consumer class. e.g. for Kafka, defaults to volley.connectors.kafka.KafkaConsumer ; cf. Extending Connectors . config Optional[Dict[Any, Any]] (Optional[str]): Any configuration to be passed directly to the queue connector. For example, all librdkafka configurations can be passed through to the connector via a dictionary here. Example \u00b6 from volley import Engine , QueueConfig input_cfg = QueueConfig ( name = \"my-input-queue\" , value = \"my.input.kafka.topic\" , profile = \"confluent\" , config = { \"consumer.group\" : \"my_consumer_group\" , \"bootstrap.servers\" : \"kafka:9092\" } ) output_cfg = QueueConfig ( name = \"my-output-queue\" , value = \"my.output.kafka.topic\" , profile = \"confluent\" , config = { \"bootstrap.servers\" : \"kafka:9092\" } ) app = Engine ( app_name = \"my-app\" , input_queue = \"my-input-queue\" , output_queues = [ \"my-output-queue\" ], queue_config = [ input_cfg , output_cfg ] ) custom_model_dump ( self ) \u00b6 replicates pydantic v1 behavior of .dict() which was broken in v2 because now pydantic tries to serialize the data_model mappingproxy Source code in volley/config.py def custom_model_dump ( self ) -> Dict [ str , Any ]: \"\"\"replicates pydantic v1 behavior of .dict() which was broken in v2 because now pydantic tries to serialize the data_model mappingproxy\"\"\" dump = self . model_dump ( exclude = \"data_model\" , exclude_unset = True ) # type: ignore # Preserve the previous behavior of not returning data model if it was not set if \"data_model\" in self . model_fields_set : dump [ \"data_model\" ] = self . data_model return dump to_dict ( self ) \u00b6 transform to dictionary omitting optional fields when not provided Source code in volley/config.py def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"transform to dictionary omitting optional fields when not provided\"\"\" return { k : v for k , v in self . __dict__ . items () if v is not None } Dict \u00b6 The queue configuration can also be composed as Dict[str, Dict[str, str]] . Each key in the dictionary represents the name (alias) for the queue, and the value is a dictionary of the queue's specific configurations and accept the same attributes as documented in the volley.QueueConfig class. queue_config = { \"my_alias_for_input_queue\" : { # alias for the queue. \"value\" : \"value_for_input_queue_name\" , # physical name for the queue \"profile\" : \"confluent\" , # kafka|rsmq \"data_model\" : \"my.models.InputMessage\" , # path to Pydantic model for validating data to/from the queue }, \"output-topic\" : { \"value\" : \"outgoing.kafka.topic\" , \"profile\" : \"confluent\" , \"data_model\" : \"my.models.OutputMessage\" }, \"dead-letter-queue\" : { \"value\" : \"deadletter.kafka.topic\" , \"profile\" : \"confluent\" }, } The configuration is passed to queue_config app = Engine ( input_queue = \"my_alias_for_queue\" , output_topics = [ \"output-topic\" ], dead_letter_queue = \"dead-letter-queue\" , queue_config = queue_config ) yaml \u00b6 Queue configuration can also be defined via a yml file. The below is equivalent to the example shown above. This can be useful when needing to configure multiple Volley workers, particularly when multiple workers may need to publish to the same queue. Define the queues in one place and import the configuration everywhere. # ./my_config.yml queues: my_alias_for_input_queue: value: value_for_input_queue_name profile: confluent data_model: my.models.InputMessage output-topic: value: outgoing.kafka.topic profile: confluent data_model: my.models.OutputMessage dead-letter-queue: value: deadletter.kafka.topic profile: confluent Then reference the file when instantiating the application. app = Engine ( input_queue = \"my_alias_for_queue\" , output_topics = [ \"output-topic\" ], dead_letter_queue = \"dead-letter-queue\" , yaml_config_path = \"./my_config.yml\" )","title":"Queues"},{"location":"queue_config/#configuration","text":"Volley's configuration centers around queues. The configurations define how you application will interact with and how Volley parses data being received/sent to each queue. Queues have names, data models, model handlers, serializers, and connectors. It is recommended to define Volley's configuration by passing a dictionary directly the Engine initializer.","title":"Configuration"},{"location":"queue_config/#queue-configuration","text":"The queue configuration object is a dictionary passed into the Volley engine at initialization. It can be defined either via a list[QueueConfig] , a dict , or a .yaml file.","title":"Queue Configuration"},{"location":"queue_config/#listqueueconfig","text":"Represents the configuration for a single queue. Provides the application with necessary configuration for interacting with the specified queue. Attributes: Name Type Description name str Alias for a particular queue. value str The system name for a queue. For example, the name of a Kafka topic (prd.my.long.kafka.topic.name) or name of a RSMQ queue. profile Optional[str] Either kafka|rsmq . Pertains to the type of connector required to produce and consume from the queue. If not provided, must provide values for ALL of consumer, producer, serializer, model_handler . data_model Optional[str] Defaults to volley.data_models.GenericMessage . Path to the Pydantic model used for data validation. When default is used, Volley will only validate that messages can be successfully converted to a Pydantic model or dictionary. serializer Optional[str] Defaults to volley.serializers.OrJsonSerializer . Path to the serializer. producer Optional[str] Used for providing a custom producer connector. Overrides the producer pertaining to that provided in type . Provide the dot path to the producer class. e.g. for Kafka,defaults to volley.connectors.kafka.KafkaProducer ; cf. Extending Connectors . consumer Optional[str] Used for providing a custom consumer connector. Overrides the consumer pertaining to that provided in type . Provide the dot path to the consumer class. e.g. for Kafka, defaults to volley.connectors.kafka.KafkaConsumer ; cf. Extending Connectors . config Optional[Dict[Any, Any]] (Optional[str]): Any configuration to be passed directly to the queue connector. For example, all librdkafka configurations can be passed through to the connector via a dictionary here.","title":"List[QueueConfig]"},{"location":"queue_config/#volley.config.QueueConfig--example","text":"from volley import Engine , QueueConfig input_cfg = QueueConfig ( name = \"my-input-queue\" , value = \"my.input.kafka.topic\" , profile = \"confluent\" , config = { \"consumer.group\" : \"my_consumer_group\" , \"bootstrap.servers\" : \"kafka:9092\" } ) output_cfg = QueueConfig ( name = \"my-output-queue\" , value = \"my.output.kafka.topic\" , profile = \"confluent\" , config = { \"bootstrap.servers\" : \"kafka:9092\" } ) app = Engine ( app_name = \"my-app\" , input_queue = \"my-input-queue\" , output_queues = [ \"my-output-queue\" ], queue_config = [ input_cfg , output_cfg ] )","title":"Example"},{"location":"queue_config/#volley.config.QueueConfig.custom_model_dump","text":"replicates pydantic v1 behavior of .dict() which was broken in v2 because now pydantic tries to serialize the data_model mappingproxy Source code in volley/config.py def custom_model_dump ( self ) -> Dict [ str , Any ]: \"\"\"replicates pydantic v1 behavior of .dict() which was broken in v2 because now pydantic tries to serialize the data_model mappingproxy\"\"\" dump = self . model_dump ( exclude = \"data_model\" , exclude_unset = True ) # type: ignore # Preserve the previous behavior of not returning data model if it was not set if \"data_model\" in self . model_fields_set : dump [ \"data_model\" ] = self . data_model return dump","title":"custom_model_dump()"},{"location":"queue_config/#volley.config.QueueConfig.to_dict","text":"transform to dictionary omitting optional fields when not provided Source code in volley/config.py def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"transform to dictionary omitting optional fields when not provided\"\"\" return { k : v for k , v in self . __dict__ . items () if v is not None }","title":"to_dict()"},{"location":"queue_config/#dict","text":"The queue configuration can also be composed as Dict[str, Dict[str, str]] . Each key in the dictionary represents the name (alias) for the queue, and the value is a dictionary of the queue's specific configurations and accept the same attributes as documented in the volley.QueueConfig class. queue_config = { \"my_alias_for_input_queue\" : { # alias for the queue. \"value\" : \"value_for_input_queue_name\" , # physical name for the queue \"profile\" : \"confluent\" , # kafka|rsmq \"data_model\" : \"my.models.InputMessage\" , # path to Pydantic model for validating data to/from the queue }, \"output-topic\" : { \"value\" : \"outgoing.kafka.topic\" , \"profile\" : \"confluent\" , \"data_model\" : \"my.models.OutputMessage\" }, \"dead-letter-queue\" : { \"value\" : \"deadletter.kafka.topic\" , \"profile\" : \"confluent\" }, } The configuration is passed to queue_config app = Engine ( input_queue = \"my_alias_for_queue\" , output_topics = [ \"output-topic\" ], dead_letter_queue = \"dead-letter-queue\" , queue_config = queue_config )","title":"Dict"},{"location":"queue_config/#yaml","text":"Queue configuration can also be defined via a yml file. The below is equivalent to the example shown above. This can be useful when needing to configure multiple Volley workers, particularly when multiple workers may need to publish to the same queue. Define the queues in one place and import the configuration everywhere. # ./my_config.yml queues: my_alias_for_input_queue: value: value_for_input_queue_name profile: confluent data_model: my.models.InputMessage output-topic: value: outgoing.kafka.topic profile: confluent data_model: my.models.OutputMessage dead-letter-queue: value: deadletter.kafka.topic profile: confluent Then reference the file when instantiating the application. app = Engine ( input_queue = \"my_alias_for_queue\" , output_topics = [ \"output-topic\" ], dead_letter_queue = \"dead-letter-queue\" , yaml_config_path = \"./my_config.yml\" )","title":"yaml"},{"location":"connectors/connectors/","text":"Initialization Configurations \u00b6 Configurations passed into Engine() init will be used in both Producer and Consumer constructors. from volley.connectors.confluent import ConfluentKafkaProducer cfg = { \"output-topic\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , \"producer\" : ConfluentKafkaProducer , \"config\" : { \"bootstrap.servers\" : \"kafka:9092\" }, } } The above code snip is roughly equivalent to: from confluent_kafka import Producer p = Producer ({ \"bootstrap.servers\" : \"kafka:9092\" }) Likewise, consumer configurations are passed to consumer initializers. cfg = { \"output-topic\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , \"consumer\" : \"volley.connectors.confluent.ConfluentKafkaConsumer\" , \"config\" : { \"bootstrap.servers\" : \"kafka:9092\" , \"group.id\" : \"myConsumerGroup\" }, } } is equivalent to... from confluent_kafka import Consumer c = Consumer ({ \"bootstrap.servers\" : \"kafka:9092\" , \"group.id\" : \"myConsumerGroup\" }) Consumer's Runtime Message Context \u00b6 Applications have the option to receive the raw runtime message context from Volley. This is accomplished by adding a reserved parameter, msg_ctx to their function definition. This can be valuable if the application needs access a lower level connector detail such as the message's Kafka partition, headers, offset, etc. In a Kafka consumer, msg_ctx is the raw confluent_kafka.Message object returned from Poll(). In RSMQ, it is simply the message id. More generally, the msg_ctx is the value of volley.data_models.QueueMessage.message_context . Take note that it is a reference to the QueueMessage , and mutating the object may cause undesired results. Receiving the msg_ctx does not change any behavior of Volley. Volley will still handle serialization and data validation according to the provided queue configuration, and if either of these fails the message will still be routed to the DLQ (if configured) or crash the application (if DLQ not configured). Example using msg_ctx in a Confluent Kafka consumer. from confluent_kafka import Message @app . stream_app def main ( message , msg_ctx : Message ): partition = msg_ctx . partition () offset = msg_ctx . offset () ... Producer Runtime Configuration \u00b6 All producer connector runtime configurations and parameters are accessible through an optional third positional in the return tuple from the Volley application. Return a dictionary of configurations and they will be passed in to the producer as key-word arguments. Some common examples of these are assigning a partition key to a specific message in a Kafka producer or a visibility delay in an RSMQ producer. # example passing partition key for a message to the Kafka producer return [ ( \"output-topic\" , message_object , { \"key\" : \"<partition_key>\" }) ] Kafka \u00b6 The Kafka producer is an implementation of Confluent's Python Producer. Some commonly used configurations are: - key: str - partition key for the given message - headers: dict - headers for the kafka message A complete set of available configurations can be found in Confluent's Producer Docs and librdkafka The following environment variables can be set in lieu of passing them as configuration: KAFKA_BROKERS - maps to bootstrap.servers KAFKA_CONSUMER_BROKERS - maps to bootstrap.servers for kafka consumers, falling back to KAFKA_BROKERS if not set KAFKA_PRODUCER_BROKERS - maps to bootstrap.servers for kafka producers, falling back to KAFKA_BROKERS if not set KAFKA_KEY - maps to sasl.username KAFKA_SECRET - maps to sasl.password Redis \u00b6 The Redis producer is an implementation of the pyRSMQ project's producer. Most commonly used configurations: - delay: float - amount of time for message to remain \"invisible\" to other consumers. The complete list of configurations can be found in pyRSMQ's sendMessage docs REDIS_HOST environment variables maps to the host parameter in RSMQ configuration.","title":"Consumer & Producer"},{"location":"connectors/connectors/#initialization-configurations","text":"Configurations passed into Engine() init will be used in both Producer and Consumer constructors. from volley.connectors.confluent import ConfluentKafkaProducer cfg = { \"output-topic\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , \"producer\" : ConfluentKafkaProducer , \"config\" : { \"bootstrap.servers\" : \"kafka:9092\" }, } } The above code snip is roughly equivalent to: from confluent_kafka import Producer p = Producer ({ \"bootstrap.servers\" : \"kafka:9092\" }) Likewise, consumer configurations are passed to consumer initializers. cfg = { \"output-topic\" : { \"profile\" : \"confluent\" , \"value\" : \"my.kafka.topic\" , \"consumer\" : \"volley.connectors.confluent.ConfluentKafkaConsumer\" , \"config\" : { \"bootstrap.servers\" : \"kafka:9092\" , \"group.id\" : \"myConsumerGroup\" }, } } is equivalent to... from confluent_kafka import Consumer c = Consumer ({ \"bootstrap.servers\" : \"kafka:9092\" , \"group.id\" : \"myConsumerGroup\" })","title":"Initialization Configurations"},{"location":"connectors/connectors/#consumers-runtime-message-context","text":"Applications have the option to receive the raw runtime message context from Volley. This is accomplished by adding a reserved parameter, msg_ctx to their function definition. This can be valuable if the application needs access a lower level connector detail such as the message's Kafka partition, headers, offset, etc. In a Kafka consumer, msg_ctx is the raw confluent_kafka.Message object returned from Poll(). In RSMQ, it is simply the message id. More generally, the msg_ctx is the value of volley.data_models.QueueMessage.message_context . Take note that it is a reference to the QueueMessage , and mutating the object may cause undesired results. Receiving the msg_ctx does not change any behavior of Volley. Volley will still handle serialization and data validation according to the provided queue configuration, and if either of these fails the message will still be routed to the DLQ (if configured) or crash the application (if DLQ not configured). Example using msg_ctx in a Confluent Kafka consumer. from confluent_kafka import Message @app . stream_app def main ( message , msg_ctx : Message ): partition = msg_ctx . partition () offset = msg_ctx . offset () ...","title":"Consumer's Runtime Message Context"},{"location":"connectors/connectors/#producer-runtime-configuration","text":"All producer connector runtime configurations and parameters are accessible through an optional third positional in the return tuple from the Volley application. Return a dictionary of configurations and they will be passed in to the producer as key-word arguments. Some common examples of these are assigning a partition key to a specific message in a Kafka producer or a visibility delay in an RSMQ producer. # example passing partition key for a message to the Kafka producer return [ ( \"output-topic\" , message_object , { \"key\" : \"<partition_key>\" }) ]","title":"Producer Runtime Configuration"},{"location":"connectors/connectors/#kafka","text":"The Kafka producer is an implementation of Confluent's Python Producer. Some commonly used configurations are: - key: str - partition key for the given message - headers: dict - headers for the kafka message A complete set of available configurations can be found in Confluent's Producer Docs and librdkafka The following environment variables can be set in lieu of passing them as configuration: KAFKA_BROKERS - maps to bootstrap.servers KAFKA_CONSUMER_BROKERS - maps to bootstrap.servers for kafka consumers, falling back to KAFKA_BROKERS if not set KAFKA_PRODUCER_BROKERS - maps to bootstrap.servers for kafka producers, falling back to KAFKA_BROKERS if not set KAFKA_KEY - maps to sasl.username KAFKA_SECRET - maps to sasl.password","title":"Kafka"},{"location":"connectors/connectors/#redis","text":"The Redis producer is an implementation of the pyRSMQ project's producer. Most commonly used configurations: - delay: float - amount of time for message to remain \"invisible\" to other consumers. The complete list of configurations can be found in pyRSMQ's sendMessage docs REDIS_HOST environment variables maps to the host parameter in RSMQ configuration.","title":"Redis"},{"location":"connectors/kafka/","text":"ConfluentKafkaConsumer \u00b6 Use for consuming a single message from a single Kafka topic. Built on confluent-kafka-python/librdkafka. Offsets are stored in librdkafka and committed according to auto.commit.interval.ms https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md Multi-topic subscription \u00b6 You may find yourself wanting to configure a single Volley worker to consume from multiple Kafka topics, each with the same message schema. You can do this by specifying the queue value as a comma separated list of topics: cfg = { \"input-topic\" : { \"profile\" : \"confluent\" , \"value\" : \"kafka.topic.0,kafka.topic.1\" , \"consumer\" : \"volley.connectors.confluent.ConfluentKafkaConsumer\" , \"config\" : { \"bootstrap.servers\" : \"kafka:9092\" , \"group.id\" : \"myConsumerGroup\" }, }, } Note: multi-topic is only supported for consumption. Volley will only be able to consume to the queue as configured above. To produce to multiple topics, specify each topic as a separate queue and publish to them. return [ ( \"output-topic-0\" , message_object ), ( \"output-topic-1\" , message_object ) ] consume ( self ) \u00b6 consumes a message from any queue. Returns a QueueMessage object on success, or None when there are no messages Source code in volley/connectors/confluent.py def consume ( # type: ignore self , ) -> Optional [ QueueMessage ]: message = self . c . poll ( self . poll_interval ) if message is None : pass elif message . error (): logger . warning ( message . error ()) message = None else : return QueueMessage ( message_context = message , message = message . value ()) on_fail ( self , message_context ) \u00b6 action to perform when serialization, or data validation has failed Source code in volley/connectors/confluent.py def on_fail ( self , message_context : Message ) -> None : logger . critical ( \"Downstream failure. Did not commit topic: %s , partition: %d , offset: %d , message: %s .\" , message_context . topic (), message_context . partition (), message_context . offset (), message_context . value (), ) if self . stop_on_failure : logger . critical ( \"Downstream failure. Stopping application.\" ) # explicitly call the connectors shutdown self . shutdown () # this send a kill command to py-volley engine to handle any other graceful shutdown procedures os . kill ( os . getpid (), signal . SIGINT ) on_success ( self , message_context ) \u00b6 stores any offsets that are able to be stored Source code in volley/connectors/confluent.py def on_success ( self , message_context : Message ) -> None : \"\"\"stores any offsets that are able to be stored\"\"\" topic = message_context . topic () partition = message_context . partition () this_offset = message_context . offset () # see if we've already committed this or a higher offset # delivery report from kafka producer are not guaranteed to be in order # that they were produced # https://github.com/confluentinc/confluent-kafka-python/issues/300#issuecomment-358416432 try : last_commit = self . last_offset [ topic ][ partition ] except KeyError : # first message from this topic-partition self . last_offset [ topic ][ partition ] = this_offset self . c . store_offsets ( message_context ) return if this_offset > last_commit : self . c . store_offsets ( message_context ) # committed according to auto.commit.interval.ms self . last_offset [ topic ][ partition ] = this_offset shutdown ( self ) \u00b6 perform some action when shutting down the application. For example, close a connection or leave a consumer group Source code in volley/connectors/confluent.py def shutdown ( self ) -> None : self . c . close () logger . info ( \"Successfully commit offsets and left consumer group %s \" , self . config . get ( \"group.id\" )) ConfluentKafkaProducer \u00b6 ConfluentKafkaProducer(queue_name: str, host: Optional[str] = None, config: Dict[str, Any] = , callback_delivery: bool = False, on_success: Optional[Callable[[Any], NoneType]] = None, on_fail: Optional[Callable[[Any], NoneType]] = None, compression_type: str = 'gzip', thread: bool = False, poll_thread_timeout: float = 1.0) produce ( self , queue_name , message , message_context , ** kwargs ) \u00b6 Publish a message to a queue Parameters: Name Type Description Default queue_name str Destination queue name. required message Any The message to publish. required message_context Any Context for the consumed message. Often a message id, or a Kafka Message object. Used for Producer callbacks to consumer. required Returns: Type Description bool status of the produce operation Source code in volley/connectors/confluent.py def produce ( self , queue_name : str , message : bytes , message_context : Any , ** kwargs : Union [ str , int ]) -> bool : self . p . produce ( key = kwargs . get ( \"key\" ), topic = queue_name , value = message , headers = kwargs . get ( \"headers\" ), callback = lambda err , msg , obj = message_context : self . acked ( err , msg , obj ), ), logger . debug ( \"Sent to topic: %s \" , queue_name ) return True shutdown ( self ) \u00b6 perform some action when shutting down the application Source code in volley/connectors/confluent.py def shutdown ( self ) -> None : self . p . flush () if self . thread : self . kill_poll_thread = True self . poll_thread . join ( self . poll_thread_timeout )","title":"ConfluentKafka"},{"location":"connectors/kafka/#confluentkafkaconsumer","text":"Use for consuming a single message from a single Kafka topic. Built on confluent-kafka-python/librdkafka. Offsets are stored in librdkafka and committed according to auto.commit.interval.ms https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md","title":"ConfluentKafkaConsumer"},{"location":"connectors/kafka/#volley.connectors.confluent.ConfluentKafkaConsumer--multi-topic-subscription","text":"You may find yourself wanting to configure a single Volley worker to consume from multiple Kafka topics, each with the same message schema. You can do this by specifying the queue value as a comma separated list of topics: cfg = { \"input-topic\" : { \"profile\" : \"confluent\" , \"value\" : \"kafka.topic.0,kafka.topic.1\" , \"consumer\" : \"volley.connectors.confluent.ConfluentKafkaConsumer\" , \"config\" : { \"bootstrap.servers\" : \"kafka:9092\" , \"group.id\" : \"myConsumerGroup\" }, }, } Note: multi-topic is only supported for consumption. Volley will only be able to consume to the queue as configured above. To produce to multiple topics, specify each topic as a separate queue and publish to them. return [ ( \"output-topic-0\" , message_object ), ( \"output-topic-1\" , message_object ) ]","title":"Multi-topic subscription"},{"location":"connectors/kafka/#volley.connectors.confluent.ConfluentKafkaConsumer.consume","text":"consumes a message from any queue. Returns a QueueMessage object on success, or None when there are no messages Source code in volley/connectors/confluent.py def consume ( # type: ignore self , ) -> Optional [ QueueMessage ]: message = self . c . poll ( self . poll_interval ) if message is None : pass elif message . error (): logger . warning ( message . error ()) message = None else : return QueueMessage ( message_context = message , message = message . value ())","title":"consume()"},{"location":"connectors/kafka/#volley.connectors.confluent.ConfluentKafkaConsumer.on_fail","text":"action to perform when serialization, or data validation has failed Source code in volley/connectors/confluent.py def on_fail ( self , message_context : Message ) -> None : logger . critical ( \"Downstream failure. Did not commit topic: %s , partition: %d , offset: %d , message: %s .\" , message_context . topic (), message_context . partition (), message_context . offset (), message_context . value (), ) if self . stop_on_failure : logger . critical ( \"Downstream failure. Stopping application.\" ) # explicitly call the connectors shutdown self . shutdown () # this send a kill command to py-volley engine to handle any other graceful shutdown procedures os . kill ( os . getpid (), signal . SIGINT )","title":"on_fail()"},{"location":"connectors/kafka/#volley.connectors.confluent.ConfluentKafkaConsumer.on_success","text":"stores any offsets that are able to be stored Source code in volley/connectors/confluent.py def on_success ( self , message_context : Message ) -> None : \"\"\"stores any offsets that are able to be stored\"\"\" topic = message_context . topic () partition = message_context . partition () this_offset = message_context . offset () # see if we've already committed this or a higher offset # delivery report from kafka producer are not guaranteed to be in order # that they were produced # https://github.com/confluentinc/confluent-kafka-python/issues/300#issuecomment-358416432 try : last_commit = self . last_offset [ topic ][ partition ] except KeyError : # first message from this topic-partition self . last_offset [ topic ][ partition ] = this_offset self . c . store_offsets ( message_context ) return if this_offset > last_commit : self . c . store_offsets ( message_context ) # committed according to auto.commit.interval.ms self . last_offset [ topic ][ partition ] = this_offset","title":"on_success()"},{"location":"connectors/kafka/#volley.connectors.confluent.ConfluentKafkaConsumer.shutdown","text":"perform some action when shutting down the application. For example, close a connection or leave a consumer group Source code in volley/connectors/confluent.py def shutdown ( self ) -> None : self . c . close () logger . info ( \"Successfully commit offsets and left consumer group %s \" , self . config . get ( \"group.id\" ))","title":"shutdown()"},{"location":"connectors/kafka/#confluentkafkaproducer","text":"ConfluentKafkaProducer(queue_name: str, host: Optional[str] = None, config: Dict[str, Any] = , callback_delivery: bool = False, on_success: Optional[Callable[[Any], NoneType]] = None, on_fail: Optional[Callable[[Any], NoneType]] = None, compression_type: str = 'gzip', thread: bool = False, poll_thread_timeout: float = 1.0)","title":"ConfluentKafkaProducer"},{"location":"connectors/kafka/#volley.connectors.confluent.ConfluentKafkaProducer.produce","text":"Publish a message to a queue Parameters: Name Type Description Default queue_name str Destination queue name. required message Any The message to publish. required message_context Any Context for the consumed message. Often a message id, or a Kafka Message object. Used for Producer callbacks to consumer. required Returns: Type Description bool status of the produce operation Source code in volley/connectors/confluent.py def produce ( self , queue_name : str , message : bytes , message_context : Any , ** kwargs : Union [ str , int ]) -> bool : self . p . produce ( key = kwargs . get ( \"key\" ), topic = queue_name , value = message , headers = kwargs . get ( \"headers\" ), callback = lambda err , msg , obj = message_context : self . acked ( err , msg , obj ), ), logger . debug ( \"Sent to topic: %s \" , queue_name ) return True","title":"produce()"},{"location":"connectors/kafka/#volley.connectors.confluent.ConfluentKafkaProducer.shutdown","text":"perform some action when shutting down the application Source code in volley/connectors/confluent.py def shutdown ( self ) -> None : self . p . flush () if self . thread : self . kill_poll_thread = True self . poll_thread . join ( self . poll_thread_timeout )","title":"shutdown()"},{"location":"connectors/overview/","text":"Connectors \u00b6 Connectors are specific implementations of producers and consumers. They handle the direct read, write, delete, or even update with a data store. Volley currently supports connectors for Kafka and RSMQ. Consumers and producers are each concrete implementations of a base class, volley.connectors.base.BaseConsumer and volley.connectors.base.BaseProducer . Inherit these two classes to get started building your own connectors. Consumers \u00b6 Consumers handle reading a message from a queue. Base class for implementing a consumer consume ( self ) \u00b6 consumes a message from any queue. Returns a QueueMessage object on success, or None when there are no messages Source code in volley/connectors/base.py @abstractmethod def consume ( self ) -> Optional [ QueueMessage ]: \"\"\"consumes a message from any queue. Returns a QueueMessage object on success, or None when there are no messages \"\"\" on_fail ( self , message_context ) \u00b6 action to perform when serialization, or data validation has failed Source code in volley/connectors/base.py @abstractmethod def on_fail ( self , message_context : Any ) -> None : \"\"\"action to perform when serialization, or data validation has failed\"\"\" on_success ( self , message_context ) \u00b6 action to take when a message has been successfully consumed. For example, delete the message that was consumed. Source code in volley/connectors/base.py @abstractmethod def on_success ( self , message_context : Any ) -> None : \"\"\"action to take when a message has been successfully consumed. For example, delete the message that was consumed. \"\"\" shutdown ( self ) \u00b6 perform some action when shutting down the application. For example, close a connection or leave a consumer group Source code in volley/connectors/base.py @abstractmethod def shutdown ( self ) -> None : \"\"\"perform some action when shutting down the application. For example, close a connection or leave a consumer group \"\"\" Producers \u00b6 Producers handle the publishing of messages to a queue. Base class for implementing a producer produce ( self , queue_name , message , message_context , ** kwargs ) \u00b6 Publish a message to a queue Parameters: Name Type Description Default queue_name str Destination queue name. required message Any The message to publish. required message_context Any Context for the consumed message. Often a message id, or a Kafka Message object. Used for Producer callbacks to consumer. required Returns: Type Description bool status of the produce operation Source code in volley/connectors/base.py @abstractmethod def produce ( self , queue_name : str , message : Any , message_context : Optional [ Any ], ** kwargs : Any ) -> bool : \"\"\"Publish a message to a queue Args: queue_name (str): Destination queue name. message (Any): The message to publish. message_context (Any): Context for the consumed message. Often a message id, or a Kafka Message object. Used for Producer callbacks to consumer. Returns: bool: status of the produce operation \"\"\" shutdown ( self ) \u00b6 perform some action when shutting down the application Source code in volley/connectors/base.py @abstractmethod def shutdown ( self ) -> None : \"\"\"perform some action when shutting down the application\"\"\" Supported Connectors \u00b6 Queues are the broker and backend that handle messages. Volley has built in support for two types of queue technologies; RSMQ and Kafka. pyRSMQ \u00b6 The python implementation of the RSMQ project. It is lightweight but full featured message queue system built on Redis. It provides clean interface to producing and consuming messages from a queue. It only supports strict FIFO - you cannot modify or update a message on the queue once it is produced. The number of messages that can exist in the queue is limited by the amount of memory available to Redis. Environment variables: REDIS_HOST = host_to_run_rsmq Kafka \u00b6 Implemented on confluent_kafka . The following configurations can be provided via environment variables: KAFKA_CONSUMER_GROUP = kafka_consumer_group KAFKA_KEY = kafka_username KAFKA_SECRET = kafka_password KAFKA_BROKERS = host:port But all librdkafka configurations can be passed through to the connector as config . The bootstrap.servers configuration is passed through in the example below. cfg = { \"output_topic\" : { \"value\" : \"output.kafka.topic.name\" , \"profile\" : \"confluent\" , \"data_model\" : \"volley.data_models.GenericMessage\" , \"config\" : { \"bootstrap.servers\" : \"kafka_broker_host:9092\" } } } Extending Connectors with Plugins \u00b6 Users can write their own connectors as needed. This is done by subclassing the corresponding Producer and Consumer , then registering them as a plugin connector in the engine configuration. The base class for consumers and producers are clearly defined: # volley/connectors/base.py # Copyright (c) Shipt, Inc. # This source code is licensed under the MIT license found in the # LICENSE file in the root directory of this source tree. from abc import ABC , abstractmethod from dataclasses import dataclass , field from typing import Any , Callable , Dict , Optional from volley.data_models import QueueMessage @dataclass class BaseConsumer ( ABC ): \"\"\"Base class for implementing a consumer\"\"\" queue_name : str host : Optional [ str ] = None config : Dict [ str , Any ] = field ( default_factory = dict ) @abstractmethod def consume ( self ) -> Optional [ QueueMessage ]: \"\"\"consumes a message from any queue. Returns a QueueMessage object on success, or None when there are no messages \"\"\" @abstractmethod def on_success ( self , message_context : Any ) -> None : \"\"\"action to take when a message has been successfully consumed. For example, delete the message that was consumed. \"\"\" @abstractmethod def on_fail ( self , message_context : Any ) -> None : \"\"\"action to perform when serialization, or data validation has failed\"\"\" @abstractmethod def shutdown ( self ) -> None : \"\"\"perform some action when shutting down the application. For example, close a connection or leave a consumer group \"\"\" @dataclass class BaseProducer ( ABC ): \"\"\"Base class for implementing a producer\"\"\" queue_name : str host : Optional [ str ] = None config : Dict [ str , Any ] = field ( default_factory = dict ) callback_delivery : bool = False on_success : Optional [ Callable [[ Any ], None ]] = None on_fail : Optional [ Callable [[ Any ], None ]] = None @abstractmethod def produce ( self , queue_name : str , message : Any , message_context : Optional [ Any ], ** kwargs : Any ) -> bool : \"\"\"Publish a message to a queue Args: queue_name (str): Destination queue name. message (Any): The message to publish. message_context (Any): Context for the consumed message. Often a message id, or a Kafka Message object. Used for Producer callbacks to consumer. Returns: bool: status of the produce operation \"\"\" @abstractmethod def shutdown ( self ) -> None : \"\"\"perform some action when shutting down the application\"\"\" Consumers receive a QueueMessage object, which has two attributes; message_context and the message itself. message_context is used for actions such as delete (delete the message with message_context from the queue), or on_fail (place the message back on the queue). Producers are simple. They publish bytes to a queue. Build a plugin \u00b6 First, let's build a Consumer and Producer for Postgres. # my_plugin.py from dataclasses import dataclass from datetime import datetime from typing import Any from sqlalchemy import ( Boolean , Column , Float , MetaData , String , Table , create_engine , text , ) from sqlalchemy.dialects.postgresql import insert from sqlalchemy.engine.base import Engine from sqlalchemy.orm import Session from sqlalchemy.sql.sqltypes import DateTime from volley.connectors.base import BaseConsumer , BaseProducer from volley.data_models import QueueMessage from volley.logging import logger def get_eng () -> Engine : connection_str = \" {} :// {} : {} @ {} : {} / {} \" . format ( \"postgresql\" , \"postgres\" , \"password\" , \"postgres\" , 5432 , \"postgres\" ) return create_engine ( connection_str , connect_args = { \"connect_timeout\" : 2 }, pool_pre_ping = True ) metadata_obj = MetaData () queue_table = Table ( \"my_long_table_name\" , metadata_obj , Column ( \"request_id\" , String ( 40 ), nullable = False ), Column ( \"max_plus\" , Float ), Column ( \"message_sent_at\" , DateTime ), Column ( \"visible\" , Boolean ), ) @dataclass class MyPGConsumer ( BaseConsumer ): def __post_init__ ( self ) -> None : self . engine : Engine = get_eng () metadata_obj . create_all ( self . engine ) self . session = Session ( self . engine ) def consume ( self ) -> QueueMessage : \"\"\"returns a random value\"\"\" sql = f \"\"\" BEGIN; WITH cte AS ( SELECT * FROM ' { self . queue_name } ' LIMIT 1 FOR UPDATE SKIP LOCKED ) UPDATE ' { self . queue_name } ' SET visible = false WHERE request_id = (select request_id from cte) RETURNING *; \"\"\" records = [ r . _mapping for r in self . session . execute ( text ( sql ))] self . session . execute ( text ( \"COMMIT;\" )) return QueueMessage ( message_context = dict ( records [ 0 ])[ \"request_id\" ], message = { \"results\" : records }) def on_success ( self , message_context : str ) -> None : self . session . execute ( text ( f \"\"\" BEGIN; DELETE FROM ' { self . queue_name } ' WHERE request_id = ' { message_context } ' AND visible = false; COMMIT; \"\"\" ) ) def on_fail ( self , message_context : str ) -> None : self . session . execute ( text ( f \"\"\" BEGIN; UPDATE ' { self . queue_name } ' SET visible = true WHERE request_id = ' { message_context } ' AND visible = false; COMMIT; \"\"\" ) ) def shutdown ( self ) -> None : self . session . close () @dataclass class MyPGProducer ( BaseProducer ): def __post_init__ ( self ) -> None : self . engine : Engine = get_eng () metadata_obj . create_all ( self . engine ) self . session = Session ( self . engine ) def produce ( self , queue_name : str , message : Any , message_context : Any , ** kwargs : Any ) -> bool : logger . info ( f \"produced message to: { queue_name =} - message= { message } \" ) vals = { \"message_sent_at\" : datetime . now (), \"request_id\" : message [ \"request_id\" ], \"max_plus\" : message [ \"max_plus\" ], } insert_stmt = insert ( queue_table ) . values ( ** vals ) with self . engine . begin () as c : c . execute ( insert_stmt ) . rowcount return True def shutdown ( self ) -> None : pass The consumer has the specific implementations for consume , on_success , on_fail , and shutdown . The producer implements produce and shutdown . Register the plugin \u00b6 Like all configuration, they can be specified in either yaml or a dict passed directly to volley.Engine (but not both). # ./my_volley_config.yml - name: postgres_queue value: pg_queue_table data_model: volley.data_models.GenericMessage model_handler: volley.models.PydanticModelHandler serializer: None producer: example.plugins.my_plugin.MyPGProducer consumer: example.plugins.my_plugin.MyPGConsumer Is is equivalent to: config = { \"postgres_queue\" : { \"value\" : \"pg_queue_table\" , \"data_model\" : \"volley.data_models.GenericMessage, \"model_handler\" : \"volley.models.PydanticModelHandler\" , \"serializer\" : \"disabled\" , \"producer\" : \"example.plugins.my_plugin.MyPGProducer\" , \"consumer\" : \"example.plugins.my_plugin.MyPGConsumer\" } } A complete example using this plugin is provided here","title":"Overview"},{"location":"connectors/overview/#connectors","text":"Connectors are specific implementations of producers and consumers. They handle the direct read, write, delete, or even update with a data store. Volley currently supports connectors for Kafka and RSMQ. Consumers and producers are each concrete implementations of a base class, volley.connectors.base.BaseConsumer and volley.connectors.base.BaseProducer . Inherit these two classes to get started building your own connectors.","title":"Connectors"},{"location":"connectors/overview/#consumers","text":"Consumers handle reading a message from a queue. Base class for implementing a consumer","title":"Consumers"},{"location":"connectors/overview/#volley.connectors.base.BaseConsumer.consume","text":"consumes a message from any queue. Returns a QueueMessage object on success, or None when there are no messages Source code in volley/connectors/base.py @abstractmethod def consume ( self ) -> Optional [ QueueMessage ]: \"\"\"consumes a message from any queue. Returns a QueueMessage object on success, or None when there are no messages \"\"\"","title":"consume()"},{"location":"connectors/overview/#volley.connectors.base.BaseConsumer.on_fail","text":"action to perform when serialization, or data validation has failed Source code in volley/connectors/base.py @abstractmethod def on_fail ( self , message_context : Any ) -> None : \"\"\"action to perform when serialization, or data validation has failed\"\"\"","title":"on_fail()"},{"location":"connectors/overview/#volley.connectors.base.BaseConsumer.on_success","text":"action to take when a message has been successfully consumed. For example, delete the message that was consumed. Source code in volley/connectors/base.py @abstractmethod def on_success ( self , message_context : Any ) -> None : \"\"\"action to take when a message has been successfully consumed. For example, delete the message that was consumed. \"\"\"","title":"on_success()"},{"location":"connectors/overview/#volley.connectors.base.BaseConsumer.shutdown","text":"perform some action when shutting down the application. For example, close a connection or leave a consumer group Source code in volley/connectors/base.py @abstractmethod def shutdown ( self ) -> None : \"\"\"perform some action when shutting down the application. For example, close a connection or leave a consumer group \"\"\"","title":"shutdown()"},{"location":"connectors/overview/#producers","text":"Producers handle the publishing of messages to a queue. Base class for implementing a producer","title":"Producers"},{"location":"connectors/overview/#volley.connectors.base.BaseProducer.produce","text":"Publish a message to a queue Parameters: Name Type Description Default queue_name str Destination queue name. required message Any The message to publish. required message_context Any Context for the consumed message. Often a message id, or a Kafka Message object. Used for Producer callbacks to consumer. required Returns: Type Description bool status of the produce operation Source code in volley/connectors/base.py @abstractmethod def produce ( self , queue_name : str , message : Any , message_context : Optional [ Any ], ** kwargs : Any ) -> bool : \"\"\"Publish a message to a queue Args: queue_name (str): Destination queue name. message (Any): The message to publish. message_context (Any): Context for the consumed message. Often a message id, or a Kafka Message object. Used for Producer callbacks to consumer. Returns: bool: status of the produce operation \"\"\"","title":"produce()"},{"location":"connectors/overview/#volley.connectors.base.BaseProducer.shutdown","text":"perform some action when shutting down the application Source code in volley/connectors/base.py @abstractmethod def shutdown ( self ) -> None : \"\"\"perform some action when shutting down the application\"\"\"","title":"shutdown()"},{"location":"connectors/overview/#supported-connectors","text":"Queues are the broker and backend that handle messages. Volley has built in support for two types of queue technologies; RSMQ and Kafka.","title":"Supported Connectors"},{"location":"connectors/overview/#pyrsmq","text":"The python implementation of the RSMQ project. It is lightweight but full featured message queue system built on Redis. It provides clean interface to producing and consuming messages from a queue. It only supports strict FIFO - you cannot modify or update a message on the queue once it is produced. The number of messages that can exist in the queue is limited by the amount of memory available to Redis. Environment variables: REDIS_HOST = host_to_run_rsmq","title":"pyRSMQ"},{"location":"connectors/overview/#kafka","text":"Implemented on confluent_kafka . The following configurations can be provided via environment variables: KAFKA_CONSUMER_GROUP = kafka_consumer_group KAFKA_KEY = kafka_username KAFKA_SECRET = kafka_password KAFKA_BROKERS = host:port But all librdkafka configurations can be passed through to the connector as config . The bootstrap.servers configuration is passed through in the example below. cfg = { \"output_topic\" : { \"value\" : \"output.kafka.topic.name\" , \"profile\" : \"confluent\" , \"data_model\" : \"volley.data_models.GenericMessage\" , \"config\" : { \"bootstrap.servers\" : \"kafka_broker_host:9092\" } } }","title":"Kafka"},{"location":"connectors/overview/#extending-connectors-with-plugins","text":"Users can write their own connectors as needed. This is done by subclassing the corresponding Producer and Consumer , then registering them as a plugin connector in the engine configuration. The base class for consumers and producers are clearly defined: # volley/connectors/base.py # Copyright (c) Shipt, Inc. # This source code is licensed under the MIT license found in the # LICENSE file in the root directory of this source tree. from abc import ABC , abstractmethod from dataclasses import dataclass , field from typing import Any , Callable , Dict , Optional from volley.data_models import QueueMessage @dataclass class BaseConsumer ( ABC ): \"\"\"Base class for implementing a consumer\"\"\" queue_name : str host : Optional [ str ] = None config : Dict [ str , Any ] = field ( default_factory = dict ) @abstractmethod def consume ( self ) -> Optional [ QueueMessage ]: \"\"\"consumes a message from any queue. Returns a QueueMessage object on success, or None when there are no messages \"\"\" @abstractmethod def on_success ( self , message_context : Any ) -> None : \"\"\"action to take when a message has been successfully consumed. For example, delete the message that was consumed. \"\"\" @abstractmethod def on_fail ( self , message_context : Any ) -> None : \"\"\"action to perform when serialization, or data validation has failed\"\"\" @abstractmethod def shutdown ( self ) -> None : \"\"\"perform some action when shutting down the application. For example, close a connection or leave a consumer group \"\"\" @dataclass class BaseProducer ( ABC ): \"\"\"Base class for implementing a producer\"\"\" queue_name : str host : Optional [ str ] = None config : Dict [ str , Any ] = field ( default_factory = dict ) callback_delivery : bool = False on_success : Optional [ Callable [[ Any ], None ]] = None on_fail : Optional [ Callable [[ Any ], None ]] = None @abstractmethod def produce ( self , queue_name : str , message : Any , message_context : Optional [ Any ], ** kwargs : Any ) -> bool : \"\"\"Publish a message to a queue Args: queue_name (str): Destination queue name. message (Any): The message to publish. message_context (Any): Context for the consumed message. Often a message id, or a Kafka Message object. Used for Producer callbacks to consumer. Returns: bool: status of the produce operation \"\"\" @abstractmethod def shutdown ( self ) -> None : \"\"\"perform some action when shutting down the application\"\"\" Consumers receive a QueueMessage object, which has two attributes; message_context and the message itself. message_context is used for actions such as delete (delete the message with message_context from the queue), or on_fail (place the message back on the queue). Producers are simple. They publish bytes to a queue.","title":"Extending Connectors with Plugins"},{"location":"connectors/overview/#build-a-plugin","text":"First, let's build a Consumer and Producer for Postgres. # my_plugin.py from dataclasses import dataclass from datetime import datetime from typing import Any from sqlalchemy import ( Boolean , Column , Float , MetaData , String , Table , create_engine , text , ) from sqlalchemy.dialects.postgresql import insert from sqlalchemy.engine.base import Engine from sqlalchemy.orm import Session from sqlalchemy.sql.sqltypes import DateTime from volley.connectors.base import BaseConsumer , BaseProducer from volley.data_models import QueueMessage from volley.logging import logger def get_eng () -> Engine : connection_str = \" {} :// {} : {} @ {} : {} / {} \" . format ( \"postgresql\" , \"postgres\" , \"password\" , \"postgres\" , 5432 , \"postgres\" ) return create_engine ( connection_str , connect_args = { \"connect_timeout\" : 2 }, pool_pre_ping = True ) metadata_obj = MetaData () queue_table = Table ( \"my_long_table_name\" , metadata_obj , Column ( \"request_id\" , String ( 40 ), nullable = False ), Column ( \"max_plus\" , Float ), Column ( \"message_sent_at\" , DateTime ), Column ( \"visible\" , Boolean ), ) @dataclass class MyPGConsumer ( BaseConsumer ): def __post_init__ ( self ) -> None : self . engine : Engine = get_eng () metadata_obj . create_all ( self . engine ) self . session = Session ( self . engine ) def consume ( self ) -> QueueMessage : \"\"\"returns a random value\"\"\" sql = f \"\"\" BEGIN; WITH cte AS ( SELECT * FROM ' { self . queue_name } ' LIMIT 1 FOR UPDATE SKIP LOCKED ) UPDATE ' { self . queue_name } ' SET visible = false WHERE request_id = (select request_id from cte) RETURNING *; \"\"\" records = [ r . _mapping for r in self . session . execute ( text ( sql ))] self . session . execute ( text ( \"COMMIT;\" )) return QueueMessage ( message_context = dict ( records [ 0 ])[ \"request_id\" ], message = { \"results\" : records }) def on_success ( self , message_context : str ) -> None : self . session . execute ( text ( f \"\"\" BEGIN; DELETE FROM ' { self . queue_name } ' WHERE request_id = ' { message_context } ' AND visible = false; COMMIT; \"\"\" ) ) def on_fail ( self , message_context : str ) -> None : self . session . execute ( text ( f \"\"\" BEGIN; UPDATE ' { self . queue_name } ' SET visible = true WHERE request_id = ' { message_context } ' AND visible = false; COMMIT; \"\"\" ) ) def shutdown ( self ) -> None : self . session . close () @dataclass class MyPGProducer ( BaseProducer ): def __post_init__ ( self ) -> None : self . engine : Engine = get_eng () metadata_obj . create_all ( self . engine ) self . session = Session ( self . engine ) def produce ( self , queue_name : str , message : Any , message_context : Any , ** kwargs : Any ) -> bool : logger . info ( f \"produced message to: { queue_name =} - message= { message } \" ) vals = { \"message_sent_at\" : datetime . now (), \"request_id\" : message [ \"request_id\" ], \"max_plus\" : message [ \"max_plus\" ], } insert_stmt = insert ( queue_table ) . values ( ** vals ) with self . engine . begin () as c : c . execute ( insert_stmt ) . rowcount return True def shutdown ( self ) -> None : pass The consumer has the specific implementations for consume , on_success , on_fail , and shutdown . The producer implements produce and shutdown .","title":"Build a plugin"},{"location":"connectors/overview/#register-the-plugin","text":"Like all configuration, they can be specified in either yaml or a dict passed directly to volley.Engine (but not both). # ./my_volley_config.yml - name: postgres_queue value: pg_queue_table data_model: volley.data_models.GenericMessage model_handler: volley.models.PydanticModelHandler serializer: None producer: example.plugins.my_plugin.MyPGProducer consumer: example.plugins.my_plugin.MyPGConsumer Is is equivalent to: config = { \"postgres_queue\" : { \"value\" : \"pg_queue_table\" , \"data_model\" : \"volley.data_models.GenericMessage, \"model_handler\" : \"volley.models.PydanticModelHandler\" , \"serializer\" : \"disabled\" , \"producer\" : \"example.plugins.my_plugin.MyPGProducer\" , \"consumer\" : \"example.plugins.my_plugin.MyPGConsumer\" } } A complete example using this plugin is provided here","title":"Register the plugin"},{"location":"connectors/rsmq/","text":"RSMQConsumer \u00b6 Handles consuming messages from Redis Simple Message Queue https://github.com/mlasevich/PyRSMQ#quick-intro-to-rsmq - pseudo example usage and configurations available on volley.Engine init: from volley import Engine config = { \"my_rsmq_queue\" : { \"consumer\" : \"volley.connectors.RSMQConsumer\" , \"serializer\" : \"volley.serializers.OrJsonSerialization\" , \"config\" : { \"host\" : \"<hostname for the redis instance>\" , \"port\" : \"port for the redis instance, defaults to 6379\" , \"vt\" : 120 , \"options\" : { \"decode_responses\" : False , } } } } app = Engine ( input_queue = \"my_rsmq_queue\" , queue_config = config , ... ) consume ( self ) \u00b6 Polls RSMQ for a single message. RSMQ consume returns a dictionary from the queue: # https://github.com/mlasevich/PyRSMQ/blob/master/README.md { message: Any - the message content rc: int - receive count - how many times this message was received ts: int - unix timestamp of when the message was originally sent id: str - message id } Returns: Type Description Optional[QueueMessage] The message body and it's RSMQ object. Source code in volley/connectors/rsmq.py def consume ( self ) -> Optional [ QueueMessage ]: \"\"\"Polls RSMQ for a single message. RSMQ consume returns a dictionary from the queue: # https://github.com/mlasevich/PyRSMQ/blob/master/README.md { message: Any - the message content rc: int - receive count - how many times this message was received ts: int - unix timestamp of when the message was originally sent id: str - message id } Returns: Optional[QueueMessage]: The message body and it's RSMQ object. \"\"\" _start = time . time () msg = ( self . queue . receiveMessage ( qname = self . queue_name , quiet = True , vt = self . config [ \"vt\" ]) . exceptions ( False ) . execute () ) _duration = time . time () - _start PROCESS_TIME . labels ( \"read\" ) . observe ( _duration ) if isinstance ( msg , dict ): if isinstance ( msg [ \"id\" ], bytes ): # message id is bytes when decode_response = False msg [ \"id\" ] = msg [ \"id\" ] . decode ( \"utf-8\" ) return QueueMessage ( message_context = msg [ \"id\" ], message = msg [ \"message\" ]) else : return None delete_message ( self , message_id ) \u00b6 wrapper function to handle retries Returns: Type Description bool Flag indicating the message was removed from the queue. Source code in volley/connectors/rsmq.py @retry ( reraise = True , stop = stop_after_attempt ( 10 ), wait = wait_exponential ( multiplier = 1 , min = 4 , max = 10 )) def delete_message ( self , message_id : str ) -> bool : \"\"\"wrapper function to handle retries Returns: Flag indicating the message was removed from the queue. \"\"\" result : bool = self . queue . deleteMessage ( qname = self . queue_name , id = message_id ) . execute () try : if result : return result else : # Second worker most likely started processing message before first worker finished. logger . warning ( \"Failed deleting message: {id} from queue: {qname} \" . format ( id = message_id , qname = self . queue_name ) ) return False except Exception as e : logger . error ( e ) # Raise the last exception once retries are exhausted raise on_fail ( self , message_context ) \u00b6 message will become visible once visibility timeout expires Source code in volley/connectors/rsmq.py def on_fail ( self , message_context : str ) -> None : \"\"\"message will become visible once visibility timeout expires\"\"\" # self.queue.changeMessageVisibility(qname=queue_name, id=message_context, vt=0) logger . error ( \"Failed producing message id %s .\" \"Message will reappear in queue ` %s ` when timeout expires\" , message_context , self . queue_name , ) on_success ( self , message_context ) \u00b6 action to take when a message has been successfully consumed. For example, delete the message that was consumed. Source code in volley/connectors/rsmq.py def on_success ( self , message_context : str ) -> None : _start = time . time () self . delete_message ( message_id = message_context ) _duration = time . time () - _start PROCESS_TIME . labels ( \"delete\" ) . observe ( _duration ) shutdown ( self ) \u00b6 perform some action when shutting down the application. For example, close a connection or leave a consumer group Source code in volley/connectors/rsmq.py def shutdown ( self ) -> None : self . queue . quit () RSMQProducer \u00b6 RSMQProducer(queue_name: str, host: Optional[str] = None, config: Dict[str, Any] = , callback_delivery: bool = False, on_success: Optional[Callable[[Any], NoneType]] = None, on_fail: Optional[Callable[[Any], NoneType]] = None) produce ( self , queue_name , message , message_context = None , ** kwargs ) \u00b6 Publish a message to a queue Parameters: Name Type Description Default queue_name str Destination queue name. required message Any The message to publish. required message_context Any Context for the consumed message. Often a message id, or a Kafka Message object. Used for Producer callbacks to consumer. None Returns: Type Description bool status of the produce operation Source code in volley/connectors/rsmq.py def produce ( self , queue_name : str , message : bytes , message_context : Optional [ Any ] = None , ** kwargs : Union [ str , int ] ) -> bool : _start = time . time () status : bool = self . send_message ( queue_name = queue_name , message = message , produce_cfg = kwargs ) _duration = time . time () - _start PROCESS_TIME . labels ( \"write\" ) . observe ( _duration ) return status send_message ( self , queue_name , message , produce_cfg ) \u00b6 wrapper function to handle retries retrying Source code in volley/connectors/rsmq.py @retry ( reraise = True , stop = stop_after_attempt ( 5 ), wait = wait_fixed ( 2 )) def send_message ( self , queue_name : str , message : bytes , produce_cfg : Dict [ str , Any ]) -> bool : \"\"\"wrapper function to handle retries retrying\"\"\" msg_id : str = self . queue . sendMessage ( qname = queue_name , message = message , encode = produce_cfg . get ( \"encode\" , False ), delay = produce_cfg . get ( \"delay\" , None ), quiet = produce_cfg . get ( \"quiet\" , False ), ) . execute () return bool ( msg_id ) shutdown ( self ) \u00b6 perform some action when shutting down the application Source code in volley/connectors/rsmq.py def shutdown ( self ) -> None : self . queue . quit ()","title":"RSMQ"},{"location":"connectors/rsmq/#rsmqconsumer","text":"Handles consuming messages from Redis Simple Message Queue https://github.com/mlasevich/PyRSMQ#quick-intro-to-rsmq - pseudo example usage and configurations available on volley.Engine init: from volley import Engine config = { \"my_rsmq_queue\" : { \"consumer\" : \"volley.connectors.RSMQConsumer\" , \"serializer\" : \"volley.serializers.OrJsonSerialization\" , \"config\" : { \"host\" : \"<hostname for the redis instance>\" , \"port\" : \"port for the redis instance, defaults to 6379\" , \"vt\" : 120 , \"options\" : { \"decode_responses\" : False , } } } } app = Engine ( input_queue = \"my_rsmq_queue\" , queue_config = config , ... )","title":"RSMQConsumer"},{"location":"connectors/rsmq/#volley.connectors.rsmq.RSMQConsumer.consume","text":"Polls RSMQ for a single message. RSMQ consume returns a dictionary from the queue: # https://github.com/mlasevich/PyRSMQ/blob/master/README.md { message: Any - the message content rc: int - receive count - how many times this message was received ts: int - unix timestamp of when the message was originally sent id: str - message id } Returns: Type Description Optional[QueueMessage] The message body and it's RSMQ object. Source code in volley/connectors/rsmq.py def consume ( self ) -> Optional [ QueueMessage ]: \"\"\"Polls RSMQ for a single message. RSMQ consume returns a dictionary from the queue: # https://github.com/mlasevich/PyRSMQ/blob/master/README.md { message: Any - the message content rc: int - receive count - how many times this message was received ts: int - unix timestamp of when the message was originally sent id: str - message id } Returns: Optional[QueueMessage]: The message body and it's RSMQ object. \"\"\" _start = time . time () msg = ( self . queue . receiveMessage ( qname = self . queue_name , quiet = True , vt = self . config [ \"vt\" ]) . exceptions ( False ) . execute () ) _duration = time . time () - _start PROCESS_TIME . labels ( \"read\" ) . observe ( _duration ) if isinstance ( msg , dict ): if isinstance ( msg [ \"id\" ], bytes ): # message id is bytes when decode_response = False msg [ \"id\" ] = msg [ \"id\" ] . decode ( \"utf-8\" ) return QueueMessage ( message_context = msg [ \"id\" ], message = msg [ \"message\" ]) else : return None","title":"consume()"},{"location":"connectors/rsmq/#volley.connectors.rsmq.RSMQConsumer.delete_message","text":"wrapper function to handle retries Returns: Type Description bool Flag indicating the message was removed from the queue. Source code in volley/connectors/rsmq.py @retry ( reraise = True , stop = stop_after_attempt ( 10 ), wait = wait_exponential ( multiplier = 1 , min = 4 , max = 10 )) def delete_message ( self , message_id : str ) -> bool : \"\"\"wrapper function to handle retries Returns: Flag indicating the message was removed from the queue. \"\"\" result : bool = self . queue . deleteMessage ( qname = self . queue_name , id = message_id ) . execute () try : if result : return result else : # Second worker most likely started processing message before first worker finished. logger . warning ( \"Failed deleting message: {id} from queue: {qname} \" . format ( id = message_id , qname = self . queue_name ) ) return False except Exception as e : logger . error ( e ) # Raise the last exception once retries are exhausted raise","title":"delete_message()"},{"location":"connectors/rsmq/#volley.connectors.rsmq.RSMQConsumer.on_fail","text":"message will become visible once visibility timeout expires Source code in volley/connectors/rsmq.py def on_fail ( self , message_context : str ) -> None : \"\"\"message will become visible once visibility timeout expires\"\"\" # self.queue.changeMessageVisibility(qname=queue_name, id=message_context, vt=0) logger . error ( \"Failed producing message id %s .\" \"Message will reappear in queue ` %s ` when timeout expires\" , message_context , self . queue_name , )","title":"on_fail()"},{"location":"connectors/rsmq/#volley.connectors.rsmq.RSMQConsumer.on_success","text":"action to take when a message has been successfully consumed. For example, delete the message that was consumed. Source code in volley/connectors/rsmq.py def on_success ( self , message_context : str ) -> None : _start = time . time () self . delete_message ( message_id = message_context ) _duration = time . time () - _start PROCESS_TIME . labels ( \"delete\" ) . observe ( _duration )","title":"on_success()"},{"location":"connectors/rsmq/#volley.connectors.rsmq.RSMQConsumer.shutdown","text":"perform some action when shutting down the application. For example, close a connection or leave a consumer group Source code in volley/connectors/rsmq.py def shutdown ( self ) -> None : self . queue . quit ()","title":"shutdown()"},{"location":"connectors/rsmq/#rsmqproducer","text":"RSMQProducer(queue_name: str, host: Optional[str] = None, config: Dict[str, Any] = , callback_delivery: bool = False, on_success: Optional[Callable[[Any], NoneType]] = None, on_fail: Optional[Callable[[Any], NoneType]] = None)","title":"RSMQProducer"},{"location":"connectors/rsmq/#volley.connectors.rsmq.RSMQProducer.produce","text":"Publish a message to a queue Parameters: Name Type Description Default queue_name str Destination queue name. required message Any The message to publish. required message_context Any Context for the consumed message. Often a message id, or a Kafka Message object. Used for Producer callbacks to consumer. None Returns: Type Description bool status of the produce operation Source code in volley/connectors/rsmq.py def produce ( self , queue_name : str , message : bytes , message_context : Optional [ Any ] = None , ** kwargs : Union [ str , int ] ) -> bool : _start = time . time () status : bool = self . send_message ( queue_name = queue_name , message = message , produce_cfg = kwargs ) _duration = time . time () - _start PROCESS_TIME . labels ( \"write\" ) . observe ( _duration ) return status","title":"produce()"},{"location":"connectors/rsmq/#volley.connectors.rsmq.RSMQProducer.send_message","text":"wrapper function to handle retries retrying Source code in volley/connectors/rsmq.py @retry ( reraise = True , stop = stop_after_attempt ( 5 ), wait = wait_fixed ( 2 )) def send_message ( self , queue_name : str , message : bytes , produce_cfg : Dict [ str , Any ]) -> bool : \"\"\"wrapper function to handle retries retrying\"\"\" msg_id : str = self . queue . sendMessage ( qname = queue_name , message = message , encode = produce_cfg . get ( \"encode\" , False ), delay = produce_cfg . get ( \"delay\" , None ), quiet = produce_cfg . get ( \"quiet\" , False ), ) . execute () return bool ( msg_id )","title":"send_message()"},{"location":"connectors/rsmq/#volley.connectors.rsmq.RSMQProducer.shutdown","text":"perform some action when shutting down the application Source code in volley/connectors/rsmq.py def shutdown ( self ) -> None : self . queue . quit ()","title":"shutdown()"},{"location":"models/BaseModelHandler/","text":"BaseModelHandler \u00b6 Base definition for a schema validator Provides the method for constructing an object with a schema construct ( self , message , schema ) \u00b6 turns a raw message into a data model The purpose is to provide the definitions on how to turn a message from a serializer into a message that an application is ready to consume. If the serializer will return a dict, then the type of the param message would be dict . The construct method would parse the dict into whatever data model that the application is expecting to received. Parameters: Name Type Description Default message Any A message to construct into a data model specified by schema . The message can be of any type so long as .construct() and .deconstruct() have implementation details to handle the type. required schema Any The data model definition. message is used to create an instance of schema required Returns: Type Description Any an instance of class schema Source code in volley/models/base.py @abstractmethod def construct ( self , message : Any , schema : Any ) -> Any : \"\"\"turns a raw message into a data model The purpose is to provide the definitions on how to turn a message from a serializer into a message that an application is ready to consume. If the serializer will return a dict, then the type of the param `message` would be `dict`. The construct method would parse the `dict` into whatever data model that the application is expecting to received. Args: message (Any): A message to construct into a data model specified by `schema`. The message can be of any type so long as .construct() and .deconstruct() have implementation details to handle the type. schema (Any): The data model definition. `message` is used to create an instance of `schema` Returns: Any: an instance of class `schema` \"\"\" deconstruct ( self , model ) \u00b6 turns data model into a raw type Source code in volley/models/base.py @abstractmethod def deconstruct ( self , model : Any ) -> Any : \"\"\"turns data model into a raw type\"\"\"","title":"BaseModelHandler"},{"location":"models/BaseModelHandler/#basemodelhandler","text":"Base definition for a schema validator Provides the method for constructing an object with a schema","title":"BaseModelHandler"},{"location":"models/BaseModelHandler/#volley.models.base.BaseModelHandler.construct","text":"turns a raw message into a data model The purpose is to provide the definitions on how to turn a message from a serializer into a message that an application is ready to consume. If the serializer will return a dict, then the type of the param message would be dict . The construct method would parse the dict into whatever data model that the application is expecting to received. Parameters: Name Type Description Default message Any A message to construct into a data model specified by schema . The message can be of any type so long as .construct() and .deconstruct() have implementation details to handle the type. required schema Any The data model definition. message is used to create an instance of schema required Returns: Type Description Any an instance of class schema Source code in volley/models/base.py @abstractmethod def construct ( self , message : Any , schema : Any ) -> Any : \"\"\"turns a raw message into a data model The purpose is to provide the definitions on how to turn a message from a serializer into a message that an application is ready to consume. If the serializer will return a dict, then the type of the param `message` would be `dict`. The construct method would parse the `dict` into whatever data model that the application is expecting to received. Args: message (Any): A message to construct into a data model specified by `schema`. The message can be of any type so long as .construct() and .deconstruct() have implementation details to handle the type. schema (Any): The data model definition. `message` is used to create an instance of `schema` Returns: Any: an instance of class `schema` \"\"\"","title":"construct()"},{"location":"models/BaseModelHandler/#volley.models.base.BaseModelHandler.deconstruct","text":"turns data model into a raw type Source code in volley/models/base.py @abstractmethod def deconstruct ( self , model : Any ) -> Any : \"\"\"turns data model into a raw type\"\"\"","title":"deconstruct()"},{"location":"models/PydanticModelHandler/","text":"PydanticModelHandler \u00b6 for pydantic model that offloads serialization to serializer construct ( self , message , schema ) \u00b6 coverts a dict to a Pydantic model Source code in volley/models/pydantic_model.py def construct ( self , message : Dict [ str , Any ], schema : Type [ BaseModelType ]) -> BaseModelType : \"\"\"coverts a dict to a Pydantic model\"\"\" return schema . model_validate ( message ) deconstruct ( self , model ) \u00b6 converts a pydantic model to a dict Source code in volley/models/pydantic_model.py def deconstruct ( self , model : BaseModelType ) -> Dict [ str , Any ]: \"\"\"converts a pydantic model to a dict\"\"\" return model . model_dump ()","title":"PydanticModelHandler"},{"location":"models/PydanticModelHandler/#pydanticmodelhandler","text":"for pydantic model that offloads serialization to serializer","title":"PydanticModelHandler"},{"location":"models/PydanticModelHandler/#volley.models.pydantic_model.PydanticModelHandler.construct","text":"coverts a dict to a Pydantic model Source code in volley/models/pydantic_model.py def construct ( self , message : Dict [ str , Any ], schema : Type [ BaseModelType ]) -> BaseModelType : \"\"\"coverts a dict to a Pydantic model\"\"\" return schema . model_validate ( message )","title":"construct()"},{"location":"models/PydanticModelHandler/#volley.models.pydantic_model.PydanticModelHandler.deconstruct","text":"converts a pydantic model to a dict Source code in volley/models/pydantic_model.py def deconstruct ( self , model : BaseModelType ) -> Dict [ str , Any ]: \"\"\"converts a pydantic model to a dict\"\"\" return model . model_dump ()","title":"deconstruct()"},{"location":"models/PydanticParserModelHandler/","text":"PydanticParserModelHandler \u00b6 pydantic model that handles serialization internally construct ( self , message , schema ) \u00b6 coverts bytes to a Pydantic model Source code in volley/models/pydantic_model.py def construct ( self , message : bytes , schema : Type [ BaseModelType ]) -> BaseModelType : \"\"\"coverts bytes to a Pydantic model\"\"\" return schema . model_validate_json ( message ) deconstruct ( self , model ) \u00b6 converts a pydantic model to bytes Source code in volley/models/pydantic_model.py def deconstruct ( self , model : BaseModelType ) -> bytes : \"\"\"converts a pydantic model to bytes\"\"\" return model . model_dump_json () . encode ( \"utf-8\" )","title":"PydanticParserModelHandler"},{"location":"models/PydanticParserModelHandler/#pydanticparsermodelhandler","text":"pydantic model that handles serialization internally","title":"PydanticParserModelHandler"},{"location":"models/PydanticParserModelHandler/#volley.models.pydantic_model.PydanticParserModelHandler.construct","text":"coverts bytes to a Pydantic model Source code in volley/models/pydantic_model.py def construct ( self , message : bytes , schema : Type [ BaseModelType ]) -> BaseModelType : \"\"\"coverts bytes to a Pydantic model\"\"\" return schema . model_validate_json ( message )","title":"construct()"},{"location":"models/PydanticParserModelHandler/#volley.models.pydantic_model.PydanticParserModelHandler.deconstruct","text":"converts a pydantic model to bytes Source code in volley/models/pydantic_model.py def deconstruct ( self , model : BaseModelType ) -> bytes : \"\"\"converts a pydantic model to bytes\"\"\" return model . model_dump_json () . encode ( \"utf-8\" )","title":"deconstruct()"},{"location":"models/data_models/","text":"GenericMessage \u00b6 The default data model for all profiles. Serves as a flexible data model. Accepts extra attributes and provides no type validation.","title":"Data models"},{"location":"models/data_models/#genericmessage","text":"The default data model for all profiles. Serves as a flexible data model. Accepts extra attributes and provides no type validation.","title":"GenericMessage"},{"location":"models/overview/","text":"Data validation \u00b6 Overview \u00b6 Model handlers live between serialization and the application and handle converting data to a model that your application is expecting to consume. Post processing from the application, they convert data to a format which can be serialized by a serialization handler. Model handlers can handle both serialization and model construction if serialization is disabled in configuration by setting serializer: None|disabled . PydanticModelHandler calls .model_validate() on the user provided Pydantic model, which takes in a dict then validates the data and creates the instance of the Pydantic model. PydanticParserModelHandler calls .parse_raw() on the user provided Pydantic model, which takes in str|bytes and parses to json before validating and creating and instance of the Pydantic model. volley.models.PydanticParserModelHandler is an example of a model handler that also conducts serialization. Example \u00b6 All model handlers inherit from BaseModelHandler . They need to construct and deconstruct a data model. To illustrate, we will use the following example: KafkaConsumer consumed message from topic as bytes: b'{\"hello\": \"world\"}' JSONSerializer converts the bytes to dict: {\"hello\":\"world\"} PydanticModelHandler is the default handler for most profiles. User creates Pydantic models for input and output data with the following definition: from pydantic import BaseModel class myIncomingData ( BaseModel ): hello : str class myOutgoingData ( BaseModel ): foo : str This model is registered in configuration: config = { \"input-queue\" :{ ... , \"data_model\" : myIncomingData }, \"output-queue\" :{ ... , \"data_model\" : myOutgoingData }, } When Volley uses the PydanticModelHandler to construct an instance of myIncomingData using the raw incoming message data. PydanticModelHandler.construct() is called with myIncomingData and the incoming message (deserialized to dict from orjson). This effectively becomes the following operation: incoming_model = myModel . model_validate ( message ) incoming_model: myModel is passed to the application for processing. The user constructs the outgoing message in their application function and returns it to Volley: out_message = myOutgoingData ( foo = \"bar\" ) return [( \"output-queue\" , out_message )] Volley then uses PydanticModelHandler to deconstruct myOutgoingData to a dict which is then passed to orjson for serialization, finally to Kafka via the connector. Extending Models \u00b6 A model handler can be defined to construct any data model so long as it is compliant with the signature of BaseModelHandler and the configured serializer is selected in configuration. To further illustrate, if one were to disable serialization and use PydanticParserModelHandler , constructing a data model would effectively become: incoming_model = myModel . parse_raw ( message ) and deconstructing the data would become: outgoing_message = outgoing_model . json () . encode ( \"utf-8\" )","title":"Overview"},{"location":"models/overview/#data-validation","text":"","title":"Data validation"},{"location":"models/overview/#overview","text":"Model handlers live between serialization and the application and handle converting data to a model that your application is expecting to consume. Post processing from the application, they convert data to a format which can be serialized by a serialization handler. Model handlers can handle both serialization and model construction if serialization is disabled in configuration by setting serializer: None|disabled . PydanticModelHandler calls .model_validate() on the user provided Pydantic model, which takes in a dict then validates the data and creates the instance of the Pydantic model. PydanticParserModelHandler calls .parse_raw() on the user provided Pydantic model, which takes in str|bytes and parses to json before validating and creating and instance of the Pydantic model. volley.models.PydanticParserModelHandler is an example of a model handler that also conducts serialization.","title":"Overview"},{"location":"models/overview/#example","text":"All model handlers inherit from BaseModelHandler . They need to construct and deconstruct a data model. To illustrate, we will use the following example: KafkaConsumer consumed message from topic as bytes: b'{\"hello\": \"world\"}' JSONSerializer converts the bytes to dict: {\"hello\":\"world\"} PydanticModelHandler is the default handler for most profiles. User creates Pydantic models for input and output data with the following definition: from pydantic import BaseModel class myIncomingData ( BaseModel ): hello : str class myOutgoingData ( BaseModel ): foo : str This model is registered in configuration: config = { \"input-queue\" :{ ... , \"data_model\" : myIncomingData }, \"output-queue\" :{ ... , \"data_model\" : myOutgoingData }, } When Volley uses the PydanticModelHandler to construct an instance of myIncomingData using the raw incoming message data. PydanticModelHandler.construct() is called with myIncomingData and the incoming message (deserialized to dict from orjson). This effectively becomes the following operation: incoming_model = myModel . model_validate ( message ) incoming_model: myModel is passed to the application for processing. The user constructs the outgoing message in their application function and returns it to Volley: out_message = myOutgoingData ( foo = \"bar\" ) return [( \"output-queue\" , out_message )] Volley then uses PydanticModelHandler to deconstruct myOutgoingData to a dict which is then passed to orjson for serialization, finally to Kafka via the connector.","title":"Example"},{"location":"models/overview/#extending-models","text":"A model handler can be defined to construct any data model so long as it is compliant with the signature of BaseModelHandler and the configured serializer is selected in configuration. To further illustrate, if one were to disable serialization and use PydanticParserModelHandler , constructing a data model would effectively become: incoming_model = myModel . parse_raw ( message ) and deconstructing the data would become: outgoing_message = outgoing_model . json () . encode ( \"utf-8\" )","title":"Extending Models"},{"location":"serializers/BaseSerialization/","text":"BaseSerialization \u00b6 Base class for serializing and deserializing queue data deserialize ( self , message ) \u00b6 deserialize from queue Source code in volley/serializers/base.py @abstractmethod def deserialize ( self , message : bytes ) -> Any : \"\"\"deserialize from queue\"\"\" serialize ( self , message ) \u00b6 serialize to queue Source code in volley/serializers/base.py @abstractmethod def serialize ( self , message : Dict [ Any , Any ]) -> bytes : \"\"\"serialize to queue\"\"\"","title":"BaseSerialization"},{"location":"serializers/BaseSerialization/#baseserialization","text":"Base class for serializing and deserializing queue data","title":"BaseSerialization"},{"location":"serializers/BaseSerialization/#volley.serializers.base.BaseSerialization.deserialize","text":"deserialize from queue Source code in volley/serializers/base.py @abstractmethod def deserialize ( self , message : bytes ) -> Any : \"\"\"deserialize from queue\"\"\"","title":"deserialize()"},{"location":"serializers/BaseSerialization/#volley.serializers.base.BaseSerialization.serialize","text":"serialize to queue Source code in volley/serializers/base.py @abstractmethod def serialize ( self , message : Dict [ Any , Any ]) -> bytes : \"\"\"serialize to queue\"\"\"","title":"serialize()"},{"location":"serializers/JsonSerialization/","text":"JsonSerialization \u00b6 deserialize ( self , message ) \u00b6 deserialize from queue Source code in volley/serializers/json_serializer.py def deserialize ( self , message : bytes ) -> Dict [ str , Any ]: deserialized : Dict [ str , Any ] = json . loads ( message ) return deserialized serialize ( self , message ) \u00b6 serialize to queue Source code in volley/serializers/json_serializer.py def serialize ( self , message : Dict [ Any , Any ]) -> bytes : serialized : bytes = json . dumps ( message , default = str ) . encode ( \"utf-8\" ) return serialized","title":"JsonSerialization"},{"location":"serializers/JsonSerialization/#jsonserialization","text":"","title":"JsonSerialization"},{"location":"serializers/JsonSerialization/#volley.serializers.json_serializer.JsonSerialization.deserialize","text":"deserialize from queue Source code in volley/serializers/json_serializer.py def deserialize ( self , message : bytes ) -> Dict [ str , Any ]: deserialized : Dict [ str , Any ] = json . loads ( message ) return deserialized","title":"deserialize()"},{"location":"serializers/JsonSerialization/#volley.serializers.json_serializer.JsonSerialization.serialize","text":"serialize to queue Source code in volley/serializers/json_serializer.py def serialize ( self , message : Dict [ Any , Any ]) -> bytes : serialized : bytes = json . dumps ( message , default = str ) . encode ( \"utf-8\" ) return serialized","title":"serialize()"},{"location":"serializers/MsgPackSerialization/","text":"MsgPackSerialization \u00b6 deserialize ( self , message ) \u00b6 deserialize from queue Source code in volley/serializers/msgpack_serializer.py def deserialize ( self , message : bytes ) -> Dict [ str , Any ]: deserialized : Dict [ str , Any ] = msgpack . unpackb ( message ) return deserialized serialize ( self , message ) \u00b6 serialize to queue Source code in volley/serializers/msgpack_serializer.py def serialize ( self , message : Dict [ Any , Any ]) -> bytes : serialized : bytes = msgpack . packb ( message ) return serialized","title":"MsgPackSerialization"},{"location":"serializers/MsgPackSerialization/#msgpackserialization","text":"","title":"MsgPackSerialization"},{"location":"serializers/MsgPackSerialization/#volley.serializers.msgpack_serializer.MsgPackSerialization.deserialize","text":"deserialize from queue Source code in volley/serializers/msgpack_serializer.py def deserialize ( self , message : bytes ) -> Dict [ str , Any ]: deserialized : Dict [ str , Any ] = msgpack . unpackb ( message ) return deserialized","title":"deserialize()"},{"location":"serializers/MsgPackSerialization/#volley.serializers.msgpack_serializer.MsgPackSerialization.serialize","text":"serialize to queue Source code in volley/serializers/msgpack_serializer.py def serialize ( self , message : Dict [ Any , Any ]) -> bytes : serialized : bytes = msgpack . packb ( message ) return serialized","title":"serialize()"},{"location":"serializers/OrJsonSerialization/","text":"OrJsonSerialization \u00b6 deserialize ( self , message ) \u00b6 deserialize from queue Source code in volley/serializers/orjson_serializer.py def deserialize ( self , message : bytes ) -> Dict [ str , Any ]: deserialized : Dict [ str , Any ] = orjson . loads ( message ) return deserialized serialize ( self , message ) \u00b6 serialize to queue Source code in volley/serializers/orjson_serializer.py def serialize ( self , message : Dict [ Any , Any ]) -> bytes : serialized : bytes = orjson . dumps ( message , option = orjson . OPT_NAIVE_UTC , default = str ) return serialized","title":"OrJsonSerialization"},{"location":"serializers/OrJsonSerialization/#orjsonserialization","text":"","title":"OrJsonSerialization"},{"location":"serializers/OrJsonSerialization/#volley.serializers.orjson_serializer.OrJsonSerialization.deserialize","text":"deserialize from queue Source code in volley/serializers/orjson_serializer.py def deserialize ( self , message : bytes ) -> Dict [ str , Any ]: deserialized : Dict [ str , Any ] = orjson . loads ( message ) return deserialized","title":"deserialize()"},{"location":"serializers/OrJsonSerialization/#volley.serializers.orjson_serializer.OrJsonSerialization.serialize","text":"serialize to queue Source code in volley/serializers/orjson_serializer.py def serialize ( self , message : Dict [ Any , Any ]) -> bytes : serialized : bytes = orjson . dumps ( message , option = orjson . OPT_NAIVE_UTC , default = str ) return serialized","title":"serialize()"},{"location":"serializers/overview/","text":"Serialization \u00b6 Volley ships with two JSON serializers and one MessagePack serializer. Supported Serializers \u00b6 JSON \u00b6 volley.serializers.OrJsonSerialization : the default serializer volley.serializers.JsonSerialization library MessagePack \u00b6 volley.serializers.MsgPackSerialization Extending Serialization with Plugins \u00b6 Serialization can be extended with plugins. Serializers must do two things: serialize and deserialize . serialize : covert a Python dict to bytes deserialize : convert bytes to a Python dict You can build a plugin by subclassing BaseSerialization from volley/serializers/base.py Register the plugin \u00b6 Like all configuration, they can be specified in either yaml or a dict passed directly to volley.engine.Engine (but not both). config = { \"my_topic\" : { \"value\" : \"my_topic_name\" , \"profile\" : \"confluent\" , \"serializer\" : \"path.to.mySerializer\" } } Or via yaml: # ./my_volley_config.yml queues: my_topic: value: my_topic_name type: kafka serializer: path.to.mySerializer","title":"Overview"},{"location":"serializers/overview/#serialization","text":"Volley ships with two JSON serializers and one MessagePack serializer.","title":"Serialization"},{"location":"serializers/overview/#supported-serializers","text":"","title":"Supported Serializers"},{"location":"serializers/overview/#json","text":"volley.serializers.OrJsonSerialization : the default serializer volley.serializers.JsonSerialization library","title":"JSON"},{"location":"serializers/overview/#messagepack","text":"volley.serializers.MsgPackSerialization","title":"MessagePack"},{"location":"serializers/overview/#extending-serialization-with-plugins","text":"Serialization can be extended with plugins. Serializers must do two things: serialize and deserialize . serialize : covert a Python dict to bytes deserialize : convert bytes to a Python dict You can build a plugin by subclassing BaseSerialization from volley/serializers/base.py","title":"Extending Serialization with Plugins"},{"location":"serializers/overview/#register-the-plugin","text":"Like all configuration, they can be specified in either yaml or a dict passed directly to volley.engine.Engine (but not both). config = { \"my_topic\" : { \"value\" : \"my_topic_name\" , \"profile\" : \"confluent\" , \"serializer\" : \"path.to.mySerializer\" } } Or via yaml: # ./my_volley_config.yml queues: my_topic: value: my_topic_name type: kafka serializer: path.to.mySerializer","title":"Register the plugin"}]}